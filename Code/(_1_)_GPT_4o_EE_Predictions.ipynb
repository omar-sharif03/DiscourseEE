{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63xqDFkgVy_K"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain-openai\n",
        "!pip install langchain\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np7h605dW4WJ"
      },
      "source": [
        "##Reading the data\n",
        "Using all the functions from previous script to read the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyBfzSYQW379",
        "outputId": "8892fbd7-cbc0-46e5-e6e5-73a4e8dabaf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XK7rUxsXP1u"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxG5kRnz8VA9",
        "outputId": "d9d69894-51ea-4d29-86a2-57c28ad44930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "246 50 100\n"
          ]
        }
      ],
      "source": [
        "#@title Reading train, dev and test data (use own folder path here and chec file names)\n",
        "folder_path = 'your folder-path'\n",
        "\n",
        "def read_json_file(name):\n",
        "    with open(name, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        return data\n",
        "\n",
        "def read_data():\n",
        "    train = read_json_file(os.path.join(folder_path, \"train.json\"))\n",
        "    dev = read_json_file(os.path.join(folder_path, \"dev.json\"))\n",
        "    test = read_json_file(os.path.join(folder_path, \"test.json\"))\n",
        "\n",
        "    return train, dev, test\n",
        "\n",
        "train, dev, test = read_data()\n",
        "print(len(train), len(dev), len(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-1Wn11blFit"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import HuggingFaceHub\n",
        "import time\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx3jy00LjGMf"
      },
      "outputs": [],
      "source": [
        "pro_token = \"use your token\"\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=pro_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBHPnb0El8We"
      },
      "source": [
        "##GPT-4o models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl-Ez_8T8JcN"
      },
      "source": [
        "### zero-shot batch extraction.\n",
        "with both batch and isolated settings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import langchain\n",
        "# langchain.debug = False"
      ],
      "metadata": {
        "id": "Ttl519ARx38l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JnvPHabj0Ab"
      },
      "outputs": [],
      "source": [
        "#create the chain for LLM to invoke\n",
        "def GPT_batch_extraction_prompt_chain(model):\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=['instruction', 'role_descriptions', 'post', 'comment', 'type'],\n",
        "        template = '''\n",
        "        ##Instruction##\n",
        "        {instruction}\n",
        "\n",
        "        ##Arguments {type}##\n",
        "        {role_descriptions}\n",
        "\n",
        "        ##Post##\n",
        "        {post}\n",
        "\n",
        "        ##Comment##\n",
        "        {comment}\n",
        "\n",
        "        ##JSON##\n",
        "        '''\n",
        "    )\n",
        "\n",
        "    prompt_chain = prompt_template | model | StrOutputParser()\n",
        "    return prompt_chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18UQM7LvmTv1"
      },
      "outputs": [],
      "source": [
        "#If one argument has multiple values separate them by commas.\n",
        "#returning the outputs by invoking the chain\n",
        "\n",
        "def getting_GPT_batch_outputs(model, data, instruction, role_details, extraction_type):\n",
        "    arg_types = ['main-arguments', 'event-specific-arguments', 'subject-effect-arguments']\n",
        "    predictions = []\n",
        "    invoke_count = 0\n",
        "    for dt in data:\n",
        "          data_sample = dt\n",
        "          post, comment, label = dt['text1'], dt['text2'], dt['label']\n",
        "          print(dt['doc_id'])\n",
        "          raw_predictions = {}\n",
        "          for arg_typ in arg_types:\n",
        "              role_descriptions = role_details[label][arg_typ] #chosing the argument details from the correspond event\n",
        "              roles = list(role_descriptions.keys())\n",
        "              input_dict = {\n",
        "                'instruction' : instruction,\n",
        "                'post' : post,\n",
        "                'comment' : comment,\n",
        "                'role_descriptions' : role_descriptions,\n",
        "                'type': extraction_type\n",
        "              }\n",
        "              prompt_chain = GPT_batch_extraction_prompt_chain(model) ##creating the prompt chain\n",
        "              while True: ##to get rid of model overload error\n",
        "                  try:\n",
        "                      output = prompt_chain.invoke(input_dict) #invoking the LLM to get prediction\n",
        "                      break\n",
        "                  except Exception as e:\n",
        "                      print(e)\n",
        "                      time.sleep(3)\n",
        "              invoke_count+=1 #Checking how many times I am doing the inference.\n",
        "              raw_predictions[arg_typ] = output\n",
        "\n",
        "              # print(label, arg_typ)\n",
        "              # pprint(raw_predictions[arg_typ])\n",
        "          data_sample['raw-predictions'] = raw_predictions ##saving the raw-prediction will refine them later\n",
        "          predictions.append(data_sample)\n",
        "    print(model, extraction_type, invoke_count)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iF_MSkiAvxs",
        "outputId": "91a06622-01b0-44af-e46a-31fb4cd361bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
            "Wall time: 8.11 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "def read_json_file(name):\n",
        "    folder_path = 'path'\n",
        "    name = os.path.join(folder_path, name)\n",
        "    with open(name, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        return data\n",
        "\n",
        "def save_json(json_data, file_name):\n",
        "    json_data = json.dumps(json_data)\n",
        "    print(file_name)\n",
        "    with open(file_name, \"w\") as json_file:\n",
        "          json_file.write(json_data)\n",
        "\n",
        "def get_predictions(model, new_data, nm_model):\n",
        "      xx = 0\n",
        "      prompt_types = ['batch']\n",
        "      exp_types = ['description_guided', 'question_guided']\n",
        "\n",
        "      predictions = {}\n",
        "      openai_api_key = \"your-key\" # this is the lab key - make sure to delete or hide eventually\n",
        "      llm_oi = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.0, model='gpt-4o-2024-05-13') #this is the chat model.\n",
        "\n",
        "      # llm_hf = HuggingFaceEndpoint(repo_id= model, temperature=0.0, max_new_tokens=128)\n",
        "\n",
        "      for p_type in prompt_types:\n",
        "          for e_type in exp_types:\n",
        "              ## getting the predefined instruction template and event_roles\n",
        "              instruction_template = read_json_file('instruction_template.json')\n",
        "              event_roles = read_json_file(f'role_definitions_{e_type}.json')\n",
        "              #instruction = instruction_template[f'{p_type}-{e_type}']\n",
        "              instruction = '''Extract the following arguments from the post and comment. Do not use more than 12 words to describe an agrument. Give arguments in JSON format and put 'null' if value not specified.'''\n",
        "              data_to_predict = new_data #reinitilizing the data again again\n",
        "\n",
        "              print(model, p_type, e_type)\n",
        "              # type of experiment question-guided or description guided\n",
        "              if e_type=='description_guided':\n",
        "                  preds = getting_GPT_batch_outputs(llm_oi, data_to_predict, instruction, event_roles, 'Descriptions')\n",
        "              else:\n",
        "                  preds = getting_GPT_batch_outputs(llm_oi, data_to_predict, instruction, event_roles, 'Questions')\n",
        "\n",
        "              current_time = datetime.now().replace(second=0, microsecond=0).time()\n",
        "              current_date = datetime.now().replace(second=0, microsecond=0).date()\n",
        "\n",
        "              nm = f'preds_{nm_model}_{p_type}_{e_type}_{current_date}_{current_time}.json'\n",
        "              save_json(preds, nm)\n",
        "\n",
        "              print(\"### \", p_type, e_type, \"###\")\n",
        "              #this is very very important\n",
        "              predictions[f'{p_type}_{e_type}'] = copy.deepcopy(preds) #saving the outputs\n",
        "\n",
        "      return predictions ## return the predictions for a model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi5XpjRBZNS6"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "model_name = ['gpt-4o'] ## for saving the models in the folder\n",
        "lm_models = ['gpt-4o-2024-05-13']\n",
        "\n",
        "i = 0\n",
        "for x in range(1, 3):\n",
        "    for model in lm_models:\n",
        "        with get_openai_callback() as cb:\n",
        "            predictions = get_predictions(model, test,  model_name[i]) ## right now I am doing predictions on dev set.\n",
        "            nm = f'{x}-{model_name[i]}_test_100_predictions.json'\n",
        "            save_json(predictions, nm)\n",
        "            folder_path = 'save-path'\n",
        "            save_json(predictions, os.path.join(folder_path, nm))\n",
        "\n",
        "        print(cb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}