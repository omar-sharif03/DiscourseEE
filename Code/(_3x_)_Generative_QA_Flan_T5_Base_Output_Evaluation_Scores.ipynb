{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np7h605dW4WJ"
      },
      "source": [
        "##Reading the data\n",
        "Using all the functions from previous script to read the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyBfzSYQW379",
        "outputId": "4fe84aea-0016-4e77-df41-cb9c3de4f2af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XK7rUxsXP1u"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obhB5vhzppxJ"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S9McKespplF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers # If you are using collab, \"!\" is required to download\n",
        "!pip install bert-score\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7kzbdLKnSoZ"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import re, string\n",
        "from google.colab import files\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FDDvw8hjpzw"
      },
      "outputs": [],
      "source": [
        "# def perform_lemmatization(text):\n",
        "\n",
        "#     # Tokenize the text into words\n",
        "#     words = word_tokenize(text)\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     stemmer = PorterStemmer()\n",
        "#     lemmatized_and_stemmed_words = [(lemmatizer.lemmatize(word), stemmer.stem(word)) for word in words]\n",
        "\n",
        "#     lemmatized_words, stemmed_words = zip(*lemmatized_and_stemmed_words)\n",
        "#     lemmatized_text = ' '.join(lemmatized_words)\n",
        "#     stemmed_text = ' '.join(stemmed_words)\n",
        "\n",
        "#     # print(\"Original text:\", text)\n",
        "#     # print(\"Lemmatized text:\", lemmatized_text)\n",
        "#     # print(\"Stemmed text:\", stemmed_text)\n",
        "#     return stemmed_text\n",
        "\n",
        "def clac_sbert_score(text1, text2):\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    embeddings1 = model.encode(text1, convert_to_tensor=True)\n",
        "    embeddings2 = model.encode(text2, convert_to_tensor=True)\n",
        "\n",
        "    cosine_score = util.cos_sim(embeddings1, embeddings2)\n",
        "    #print(cosine_score.item())\n",
        "    return cosine_score.item()\n",
        "\n",
        "def calc_cosine_similarity(text1, text2):\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # text1 = perform_lemmatization(text1)\n",
        "    # text2 = perform_lemmatization(text2)\n",
        "\n",
        "    inputs1 = tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs2 = tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    outputs1 = model(**inputs1)\n",
        "    outputs2 = model(**inputs2)\n",
        "\n",
        "    embeddings1 = outputs1.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "    embeddings2 = outputs2.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "    #Calculate cosine similarity\n",
        "    similarity = np.dot(embeddings1, embeddings2.T) / (np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2))\n",
        "\n",
        "    #print(\"Similarity between the texts:\", similarity)\n",
        "    return similarity[0][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15QAAzsHqISQ"
      },
      "outputs": [],
      "source": [
        "def is_null_or_empty(string):\n",
        "    return string is None or string.strip().lower() == 'null' or string.strip() == ''\n",
        "\n",
        "def normalize_string(s):\n",
        "      def remove_articles(text):\n",
        "          regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "          return re.sub(regex, ' ', text)\n",
        "      def white_space_fix(text):\n",
        "          return ' '.join(text.split())\n",
        "      def remove_punc(text):\n",
        "          exclude = set(string.punctuation)\n",
        "          return ''.join(ch for ch in text if ch not in exclude)\n",
        "      def lower(text):\n",
        "          return text.lower()\n",
        "      return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def calculate_score(predictions, ground_truth_labels, similarity_threshold):\n",
        "    true_pos, pred_count, gtruth_count = 0, len(predictions), len(ground_truth_labels)\n",
        "\n",
        "    if ((len(ground_truth_labels)==1 and is_null_or_empty(ground_truth_labels[0]))): #only not calculating when ground-truth is null\n",
        "        #print(\"xxx \", predictions, labels) #(len(predictions)==1 and is_null_or_empty(predictions[0]))\n",
        "        return 0, 0, 0 #skipping this sample\n",
        "    else:\n",
        "        actual_labels = copy.deepcopy(ground_truth_labels)\n",
        "        # print(predictions, actual_labels)\n",
        "        # print(len(predictions), len(actual_labels))\n",
        "        for p in predictions:\n",
        "            for g in actual_labels:\n",
        "                if(g.strip() == ''): gtruth_count -=1 ## if any ground-truth is empty reducing that count.\n",
        "                if(is_null_or_empty(p) or is_null_or_empty(g)): continue ##if prediction/ground-truth is null then skip\n",
        "                np, ng = normalize_string(p), normalize_string(g)\n",
        "                sim_value = calc_cosine_similarity(np,ng)\n",
        "                sbert_score = clac_sbert_score(np, ng)\n",
        "                print(np, ng, sim_value, sbert_score)\n",
        "                if(sim_value>similarity_threshold or sbert_score>similarity_threshold):\n",
        "                    true_pos += 1\n",
        "                    actual_labels.remove(g)\n",
        "        # print(predictions, labels)\n",
        "        # print(true_pos, pred_count, gtruth_count)\n",
        "        return true_pos, pred_count, gtruth_count\n",
        "\n",
        "def calc_precision_recall_f1(true_pos, pred_count, gtruth_count):\n",
        "    print(true_pos, pred_count, gtruth_count)\n",
        "    precision = true_pos/(pred_count + 1e-10)\n",
        "    recall = true_pos/(gtruth_count + 1e-10)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-10)\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jy72FGPqKoG"
      },
      "outputs": [],
      "source": [
        "def select_event_specific_data(event, data):\n",
        "    ev_data = []\n",
        "    for dt in data:\n",
        "        if event==dt['label']:\n",
        "            ev_data.append(dt)\n",
        "    return ev_data\n",
        "\n",
        "def calc_results(all_data, event_types, arg_types, events_roles, pred_name, gtruth_name, similarity_threshold):\n",
        "    results = {}\n",
        "    for event in event_types:\n",
        "        results[event] = {}\n",
        "        for arg_type in arg_types:\n",
        "            results[event][arg_type] = {}\n",
        "            arguments = list(events_roles[event][arg_type].keys())\n",
        "            for arg in arguments:\n",
        "                tp, pc, gc = 0, 0, 0 #initialize the true_positive, pred_count, ground_truth-count for each arguments\n",
        "                event_specifc_data = select_event_specific_data(event, all_data) ##geting the data for a specific event type.\n",
        "                for dt in event_specifc_data:\n",
        "                    label = dt['label']\n",
        "                    pred_list = dt[pred_name][arg_type][arg] #predictions\n",
        "                    gtruth_list = dt[gtruth_name][arg_type][arg] #ground-truth arguments\n",
        "                    #print(pred_list, gtruth_list)\n",
        "                    true_pos, pred_count, gtruth_count = 0, 0, 0\n",
        "                    try:\n",
        "                       true_pos, pred_count, gtruth_count = calculate_score(pred_list,\n",
        "                                                                         gtruth_list, similarity_threshold)\n",
        "                    except:\n",
        "                        print(event, arg_type, arg)\n",
        "                        print(pred_list, gtruth_list)\n",
        "                        print(\"Error occured at\", dt['doc_id'])\n",
        "\n",
        "                    tp+= true_pos\n",
        "                    pc+= pred_count\n",
        "                    gc+= gtruth_count\n",
        "                if(gc==0):\n",
        "                    continue #skiping those values, where we do not have any ground truth.\n",
        "\n",
        "                precision, recall, f1 = calc_precision_recall_f1(tp, pc, gc)\n",
        "                print(event, arg_type, arg, precision, recall, f1)\n",
        "                #saving the values in two items is a list\n",
        "                #first item has (precision, recall, f1 scores) #second item has (true_pos, pred_count, ground_truth_count values)\n",
        "                results[event][arg_type][arg]= [[precision, recall, f1], [tp, pc, gc]]\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2h0znuR6wqg"
      },
      "outputs": [],
      "source": [
        "def read_json_file(name):\n",
        "    with open(name, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        return data\n",
        "\n",
        "def save_json(json_data, file_name):\n",
        "    json_data = json.dumps(json_data)\n",
        "    print(file_name)\n",
        "    with open(file_name, \"w\") as json_file:\n",
        "          json_file.write(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqvcGyz7qQtU",
        "outputId": "2a34ff7a-fc74-4158-89c4-446ad36400f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['doc_id', 'id', 'text1', 'text2', 'label', 'thread-length', 'ground-truth-arguments', 'predictions'])\n",
            "100\n"
          ]
        }
      ],
      "source": [
        "events_roles = read_json_file(os.path.join('role_definitions.json'))\n",
        "\n",
        "\n",
        "event_types = ['taking-moud', 'tapering', 'relapse']\n",
        "arg_types = ['main-arguments', 'event-specific-arguments', 'subject-effect-arguments']\n",
        "similarity_threshold = [0.749, 0.99]\n",
        "\n",
        "number = '2'\n",
        "name = 'generative-qa-flan-t5-base'\n",
        "folder_path = 'Argument-Extraction-Predictions'\n",
        "file_name = f'{number}-{name}-predictions.json'\n",
        "all_data = read_json_file(os.path.join(folder_path, file_name))\n",
        "\n",
        "print(all_data[0].keys())\n",
        "\n",
        "print(len(all_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo0flhz762TL",
        "outputId": "e92e231d-b605-4793-ebc3-137f88dfe126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'main-arguments': {'subject/patient': ['Individual altered treatment due to withdrawals'], 'effects': ['Withdrawals in the mornings'], 'treatment': ['Timing dose later', 'splitting dose 12 hrs apart']}, 'event-specific-arguments': {'medications': ['Suboxone'], 'dosage': ['null'], 'treatment-duration': ['null'], 'manner': ['orally'], 'frequency': ['20-24hr', 'sometimes 12hr'], 'timing': ['null'], 'purpose': ['Avoiding withdrawals']}, 'subject-effect-arguments': {'age': ['null'], 'gender': ['null'], 'conditions': ['null'], 'side-effects': ['withdrawals'], 'severity': ['worst'], 'start-time': ['in the morning'], 'side-effect-duration': ['null'], 'intervention': ['splitting dosage and changing frequency']}}\n",
            "{'main-arguments': {'subject/patient': ['Individual who wakes up every morning'], 'treatment': ['Taking a dose 12 hours apart'], 'effects': ['null']}, 'event-specific-arguments': {'medications': ['null'], 'dosage': ['null'], 'treatment-duration': ['null'], 'manner': ['orally'], 'frequency': ['null'], 'timing': ['null'], 'purpose': ['null']}, 'subject-effect-arguments': {'age': ['null'], 'gender': ['null'], 'conditions': ['null'], 'side-effects': ['butterflies', 'anxiety', 'crippling me every morning'], 'severity': ['severe'], 'start-time': ['null'], 'side-effect-duration': ['null'], 'intervention': ['splitting dose 12 hours apart']}}\n"
          ]
        }
      ],
      "source": [
        "print(all_data[0]['ground-truth-arguments'])\n",
        "print(all_data[0]['predictions'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "psqYG1ptqpW4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "g_name = 'ground-truth-arguments'\n",
        "p_name = 'predictions'\n",
        "numbers = ['0', '1', '2']\n",
        "name = 'generative-qa-flan-t5-base'\n",
        "folder_path = 'Argument-Extraction-Predictions'\n",
        "\n",
        "for number in numbers:\n",
        "    results_dict = {}\n",
        "    file_name = f'{number}-{name}-predictions.json'\n",
        "    all_data = read_json_file(os.path.join(folder_path, file_name))\n",
        "    for s_t in similarity_threshold:\n",
        "        results = calc_results(all_data, event_types, arg_types, events_roles,\n",
        "                  p_name, g_name, s_t)\n",
        "        save_json(results, f'{number}-{name}-{s_t}.json')\n",
        "        results_dict[f'{number}-{name}-{s_t}']=results\n",
        "\n",
        "    result_folder_path = 'Argument-Extraction-Results'\n",
        "    save_json(results_dict, os.path.join(result_folder_path, f'{number}-new-results-{name}.json'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODG8lO5PrMAk",
        "outputId": "f1b3594b-aba5-4015-ffde-829693b70e6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'taking-moud': {'main-arguments': {'subject/patient': [[0.0, 0.0, 0.0],\n",
              "    [0, 40, 43]],\n",
              "   'treatment': [[0.0, 0.0, 0.0], [0, 39, 41]],\n",
              "   'effects': [[0.04347826086937619, 0.03333333333322223, 0.03773584900733357],\n",
              "    [1, 23, 30]]},\n",
              "  'event-specific-arguments': {'medications': [[0.057142857142693876,\n",
              "     0.04651162790686857,\n",
              "     0.05128205123244576],\n",
              "    [2, 35, 43]],\n",
              "   'dosage': [[0.13043478260812855, 0.11538461538417161, 0.12244897954152437],\n",
              "    [3, 23, 26]],\n",
              "   'treatment-duration': [[0.24999999999791667,\n",
              "     0.24999999999791667,\n",
              "     0.24999999994791666],\n",
              "    [3, 12, 12]],\n",
              "   'manner': [[0.0, 0.0, 0.0], [0, 20, 21]],\n",
              "   'frequency': [[0.04761904761882087,\n",
              "     0.04545454545433884,\n",
              "     0.04651162785678746],\n",
              "    [1, 21, 22]],\n",
              "   'timing': [[0.0, 0.0, 0.0], [0, 3, 3]],\n",
              "   'purpose': [[0.0, 0.0, 0.0], [0, 23, 23]]},\n",
              "  'subject-effect-arguments': {'gender': [[0.0, 0.0, 0.0], [0, 1, 1]],\n",
              "   'conditions': [[0.0, 0.0, 0.0], [0, 9, 11]],\n",
              "   'side-effects': [[0.07692307692278107,\n",
              "     0.07407407407379973,\n",
              "     0.07547169806294056],\n",
              "    [2, 26, 27]],\n",
              "   'severity': [[0.0, 0.0, 0.0], [0, 15, 13]],\n",
              "   'start-time': [[0.0, 0.0, 0.0], [0, 6, 6]],\n",
              "   'side-effect-duration': [[0.3333333333222222,\n",
              "     0.3333333333222222,\n",
              "     0.3333333332722222],\n",
              "    [1, 3, 3]],\n",
              "   'intervention': [[0.0, 0.0, 0.0], [0, 23, 26]]}},\n",
              " 'tapering': {'main-arguments': {'subject/patient': [[0.03846153846139053,\n",
              "     0.039999999999840004,\n",
              "     0.03921568622437525],\n",
              "    [1, 26, 25]],\n",
              "   'effects': [[0.13333333333244443, 0.0999999999995, 0.11428571423608161],\n",
              "    [2, 15, 20]],\n",
              "   'tapering-event': [[0.0, 0.0, 0.0], [0, 26, 25]]},\n",
              "  'event-specific-arguments': {'condition': [[0.0, 0.0, 0.0], [0, 10, 10]],\n",
              "   'trigger': [[0.0, 0.0, 0.0], [0, 7, 7]],\n",
              "   'start-time': [[0.0, 0.0, 0.0], [0, 7, 7]],\n",
              "   'type': [[0.0, 0.0, 0.0], [0, 23, 23]],\n",
              "   'taper-medications': [[0.05882352941141869,\n",
              "     0.04545454545433884,\n",
              "     0.051282051232610135],\n",
              "    [1, 17, 22]],\n",
              "   'initial-dosage': [[0.1538461538449704,\n",
              "     0.199999999998,\n",
              "     0.17391304342759925],\n",
              "    [2, 13, 10]],\n",
              "   'current-dosage': [[0.06249999999960938,\n",
              "     0.06666666666622222,\n",
              "     0.06451612898189386],\n",
              "    [1, 16, 15]],\n",
              "   'goal-dosage': [[0.04545454545433884,\n",
              "     0.04761904761882087,\n",
              "     0.04651162785678746],\n",
              "    [1, 22, 21]],\n",
              "   'target-duration': [[0.0, 0.0, 0.0], [0, 5, 5]]},\n",
              "  'subject-effect-arguments': {'age': [[0.9999999999,\n",
              "     0.9999999999,\n",
              "     0.99999999985],\n",
              "    [1, 1, 1]],\n",
              "   'gender': [[0.0, 0.0, 0.0], [0, 2, 2]],\n",
              "   'conditions': [[0.0, 0.0, 0.0], [0, 7, 7]],\n",
              "   'side-effects': [[0.23529411764567476,\n",
              "     0.17391304347750475,\n",
              "     0.19999999995012502],\n",
              "    [4, 17, 23]],\n",
              "   'severity': [[0.0, 0.0, 0.0], [0, 10, 8]],\n",
              "   'start-time': [[0.0, 0.0, 0.0], [0, 6, 6]],\n",
              "   'side-effect-duration': [[0.6666666666444444,\n",
              "     0.6666666666444444,\n",
              "     0.6666666665944444],\n",
              "    [2, 3, 3]],\n",
              "   'intervention': [[0.0, 0.0, 0.0], [0, 14, 16]]}},\n",
              " 'relapse': {'main-arguments': {'subject/patient': [[0.0, 0.0, 0.0],\n",
              "    [0, 35, 36]],\n",
              "   'relapse-event': [[0.0, 0.0, 0.0], [0, 29, 39]],\n",
              "   'resuming-moud-after-relapse': [[0.03333333333322223,\n",
              "     0.03571428571415817,\n",
              "     0.034482758570630205],\n",
              "    [1, 30, 28]],\n",
              "   'effects': [[0.11111111111049383, 0.09090909090867769, 0.09999999994999999],\n",
              "    [2, 18, 22]]},\n",
              "  'event-specific-arguments': {'condition': [[0.0, 0.0, 0.0], [0, 25, 28]],\n",
              "   'trigger': [[0.0, 0.0, 0.0], [0, 12, 13]],\n",
              "   'existing/current-medications': [[0.299999999999,\n",
              "     0.24999999999930556,\n",
              "     0.2727272726768595],\n",
              "    [9, 30, 36]],\n",
              "   'relapse-substance': [[0.34374999999892575,\n",
              "     0.26829268292617486,\n",
              "     0.3013698629636329],\n",
              "    [11, 32, 41]],\n",
              "   'relapse-duration': [[0.09090909090826446,\n",
              "     0.08333333333263888,\n",
              "     0.08695652168846879],\n",
              "    [1, 11, 12]],\n",
              "   'relapse-intervention': [[0.05263157894709142,\n",
              "     0.039999999999840004,\n",
              "     0.04545454540526861],\n",
              "    [1, 19, 25]],\n",
              "   'waiting-time': [[0.36363636363305785,\n",
              "     0.3076923076899408,\n",
              "     0.33333333328090275],\n",
              "    [4, 11, 13]]},\n",
              "  'subject-effect-arguments': {'age': [[0.3333333333222222,\n",
              "     0.499999999975,\n",
              "     0.399999999936],\n",
              "    [1, 3, 2]],\n",
              "   'gender': [[0.3333333333222222, 0.3333333333222222, 0.3333333332722222],\n",
              "    [1, 3, 3]],\n",
              "   'conditions': [[0.06666666666622222,\n",
              "     0.07142857142806122,\n",
              "     0.06896551719096314],\n",
              "    [1, 15, 14]],\n",
              "   'side-effects': [[0.13043478260812855,\n",
              "     0.12499999999947918,\n",
              "     0.12765957441756454],\n",
              "    [3, 23, 24]],\n",
              "   'severity': [[0.0, 0.0, 0.0], [0, 10, 9]],\n",
              "   'start-time': [[0.0, 0.0, 0.0], [0, 6, 6]],\n",
              "   'side-effect-duration': [[0.399999999992, 0.399999999992, 0.399999999942],\n",
              "    [2, 5, 5]],\n",
              "   'intervention': [[0.14814814814759947,\n",
              "     0.11111111111080246,\n",
              "     0.12698412693474426],\n",
              "    [4, 27, 36]]}}}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5L-esuHuaVx"
      },
      "outputs": [],
      "source": [
        "def print_check_score(data):\n",
        "    event_types = ['taking-moud', 'relapse', 'tapering']\n",
        "    arg_types = ['main-arguments', 'event-specific-arguments', 'subject-effect-arguments']\n",
        "\n",
        "    scores = {}\n",
        "    ls3 = []\n",
        "    for event in event_types:\n",
        "        ls1 = []\n",
        "        for arg_type in arg_types:\n",
        "            arguments = list(data[event][arg_type].keys())\n",
        "            ls = []\n",
        "            for arg in arguments:\n",
        "                f1_score = data[event][arg_type][arg][0][2] ##stored f1-score\n",
        "                gt_count = data[event][arg_type][arg][1][2] ##stored ground-truth count\n",
        "                if(gt_count>5):\n",
        "                    print(event, arg_type, arg, f1_score)\n",
        "                    ls.append(f1_score)\n",
        "            if(len(ls)>0):\n",
        "                print(event, arg_type, sum(ls)/len(ls))\n",
        "                scores[f'{event}_{arg_type}_average'] = sum(ls)/len(ls)\n",
        "                ls1.append(sum(ls)/len(ls))\n",
        "        if(len(ls1)>0):\n",
        "            print(\"{:.3f}\".format(sum(ls1)/len(ls1),\"\\n\"))\n",
        "            scores[f'{event}_average'] = sum(ls1)/len(ls1)\n",
        "            ls3.append(sum(ls1)/len(ls1))\n",
        "    print(\"\\nOverall\", sum(ls3)/len(ls3))\n",
        "    pprint(scores)\n",
        "    return scores\n",
        "print_check_score(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Elc9BY2Pul3W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}