{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63xqDFkgVy_K"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain-openai\n",
        "!pip install langchain\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np7h605dW4WJ"
      },
      "source": [
        "##Reading the data\n",
        "Using all the functions from previous script to read the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyBfzSYQW379",
        "outputId": "0ddc2a67-eda7-47b8-d2ec-83404b0655f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XK7rUxsXP1u"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxG5kRnz8VA9",
        "outputId": "a98f98dc-5510-4a49-88b3-57b97a825851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "246 50 100\n"
          ]
        }
      ],
      "source": [
        "#@title Reading train, dev and test data (use own folder path here and chec file names)\n",
        "folder_path = 'your folder-path'\n",
        "\n",
        "def read_json_file(name):\n",
        "    with open(name, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        return data\n",
        "\n",
        "def read_data():\n",
        "    train = read_json_file(os.path.join(folder_path, \"train.json\"))\n",
        "    dev = read_json_file(os.path.join(folder_path, \"dev.json\"))\n",
        "    test = read_json_file(os.path.join(folder_path, \"test.json\"))\n",
        "\n",
        "    return train, dev, test\n",
        "\n",
        "train, dev, test = read_data()\n",
        "print(len(train), len(dev), len(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-1Wn11blFit"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import HuggingFaceHub\n",
        "import time\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx3jy00LjGMf"
      },
      "outputs": [],
      "source": [
        "pro_token = \"use your token\"\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=pro_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl-Ez_8T8JcN"
      },
      "source": [
        "### zero-shot batch extraction.\n",
        "with both question-guided and description-guided settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JnvPHabj0Ab"
      },
      "outputs": [],
      "source": [
        "#create the chain for LLM to invoke\n",
        "def Gemma_batch_extraction_prompt_chain(model):\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=['instruction', 'role_descriptions', 'post', 'comment', 'type'],\n",
        "        template = '''\n",
        "        [INST]\n",
        "        ##Instruction##\n",
        "        {instruction}\n",
        "\n",
        "        ##Post##\n",
        "        {post}\n",
        "\n",
        "        ##Comment##\n",
        "        {comment}\n",
        "\n",
        "        ##Arguments {type}##\n",
        "        {role_descriptions}\n",
        "        [/INST]\n",
        "        Do not use more than 12 words to describe an argument. Return \"null\" if any arugment is not present. Return arguments in JSON. Precisely give the output, no extra description is needed.\n",
        "        Provide the output between [##JSON##] [##JSON##].\n",
        "        '''\n",
        "    )\n",
        "\n",
        "    prompt_chain = prompt_template | model | StrOutputParser()\n",
        "    return prompt_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18UQM7LvmTv1"
      },
      "outputs": [],
      "source": [
        "#If one argument has multiple values separate them by commas.\n",
        "#returning the outputs by invoking the chain\n",
        "\n",
        "def getting_Gemma_batch_outputs(model, data, instruction, role_details, extraction_type):\n",
        "    arg_types = ['main-arguments', 'event-specific-arguments', 'subject-effect-arguments']\n",
        "    predictions = []\n",
        "    invoke_count = 0\n",
        "    i = 0\n",
        "    for dt in data:\n",
        "          data_sample = dt\n",
        "          post, comment, label = dt['text1'], dt['text2'], dt['label']\n",
        "          print(i, dt['doc_id'])\n",
        "          raw_predictions = {}\n",
        "          for arg_typ in arg_types:\n",
        "              role_descriptions = role_details[label][arg_typ] #chosing the argument details from the correspond event\n",
        "              roles = list(role_descriptions.keys())\n",
        "              input_dict = {\n",
        "                'instruction' : instruction,\n",
        "                'post' : post,\n",
        "                'comment' : comment,\n",
        "                'role_descriptions' : role_descriptions,\n",
        "                'type': extraction_type\n",
        "              }\n",
        "              prompt_chain = Gemma_batch_extraction_prompt_chain(model) ##creating the prompt chain\n",
        "              while True: ## to get rid of model overload error\n",
        "                  try:\n",
        "                      output = prompt_chain.invoke(input_dict)\n",
        "                      break\n",
        "                  except Exception as e:\n",
        "                      print(e)\n",
        "                      time.sleep(3)\n",
        "              invoke_count+=1 #Checking how many times I am doing the inference.\n",
        "              # print(label, arg_typ)\n",
        "              # pprint(output)\n",
        "              raw_predictions[arg_typ] = output\n",
        "\n",
        "          data_sample['raw-predictions'] = raw_predictions ##saving the raw-prediction will refine them later\n",
        "          predictions.append(data_sample)\n",
        "          i+=1\n",
        "\n",
        "    print(model, extraction_type, invoke_count)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iF_MSkiAvxs",
        "outputId": "1d50185c-2b63-4720-d716-24671f106d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
            "Wall time: 11 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "def read_json_file(name):\n",
        "    folder_path = 'your-path'\n",
        "    name = os.path.join(folder_path, name)\n",
        "    with open(name, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        return data\n",
        "\n",
        "def save_json(json_data, file_name):\n",
        "    json_data = json.dumps(json_data)\n",
        "    print(file_name)\n",
        "    with open(file_name, \"w\") as json_file:\n",
        "          json_file.write(json_data)\n",
        "\n",
        "def get_predictions(model, data_to_predict, nm_model):\n",
        "      exp_types = ['description_guided', 'question_guided']\n",
        "      prompt_types = ['batch']\n",
        "\n",
        "      predictions = {}\n",
        "      llm_hf = HuggingFaceEndpoint(repo_id= model, temperature=0.01, max_new_tokens=128)\n",
        "\n",
        "      for p_type in prompt_types:\n",
        "          for e_type in exp_types:\n",
        "              ## getting the predefined instruction template and event_roles\n",
        "              instruction_template = read_json_file('instruction_template.json')\n",
        "              event_roles = read_json_file(f'role_definitions_{e_type}.json')\n",
        "              instruction = instruction_template[f'{p_type}-{e_type}']\n",
        "\n",
        "              print(model, p_type, e_type)\n",
        "              # type of experiment question-guided or description guided\n",
        "\n",
        "              if e_type=='description_guided':\n",
        "                  preds = getting_Gemma_batch_outputs(llm_hf, data_to_predict, instruction, event_roles, 'Descriptions')\n",
        "              else:\n",
        "                  preds = getting_Gemma_batch_outputs(llm_hf, data_to_predict, instruction, event_roles, 'Questions')\n",
        "\n",
        "              current_time = datetime.now().replace(second=0, microsecond=0).time()\n",
        "              current_date = datetime.now().replace(second=0, microsecond=0).date()\n",
        "              save_json(preds, f'preds_{nm_model}_{p_type}_{e_type}_{current_date}_{current_time}.json')\n",
        "\n",
        "              #this is very very important\n",
        "              predictions[f'{p_type}_{e_type}'] = copy.deepcopy(preds) #saving the outputs\n",
        "      return predictions ## return the predictions for a model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi5XpjRBZNS6"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "lm_models = ['google/gemma-1.1-7b-it'] ## change this with huggingface model string that you want to use\n",
        "model_name = ['gemma-1.1-7b-it'] ## for saving the models in the folder\n",
        "\n",
        "i=0\n",
        "for x in range(1,3):\n",
        "    for model in lm_models:\n",
        "        predictions = get_predictions(model, test,  model_name[i]) ## right now I am doing predictions on dev set.\n",
        "        nm = f'{x}-{model_name[i]}_test_100_predictions.json'\n",
        "        save_json(predictions, nm)\n",
        "        folder_path = 'Argument-Extraction-Predictions' #Choose your path\n",
        "        save_json(predictions, os.path.join(folder_path, nm))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDTA0Gm4OCgK"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}