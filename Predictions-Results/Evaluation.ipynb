{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** In this notebook you will see `'main arguments`', `'event-specific arguments'` these indicates `'core-arguments'` and `'type-specific-arguments'` of the main paper.\n"
      ],
      "metadata": {
        "id": "R8uinLIXHLR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U-OyHq2nG7B1"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import pandas as pdar\n",
        "from pprint import pprint\n",
        "import re, string\n",
        "from google.colab import files\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##cloing the git repo\n",
        "!git clone https://omar-sharif03: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@github.com/omar-sharif03/DiscourseEE.git\n",
        "!cd /content/DiscourseEE && git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cNq-2scB-1F",
        "outputId": "70475c22-a5c0-42e0-e37e-66927ec6470e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DiscourseEE'...\n",
            "remote: Enumerating objects: 166, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
            "remote: Total 166 (delta 86), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (166/166), 1.48 MiB | 2.79 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Event-Detection-Results"
      ],
      "metadata": {
        "id": "xOzV_1NGD8DW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Models\n",
        "(BERT, RoBERTa, and MPNet)\n"
      ],
      "metadata": {
        "id": "UZo2rUReEBFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install transformers datasets evaluate"
      ],
      "metadata": {
        "id": "lJyqloaaCNel"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##This might not be required for you based on your system\n",
        "!pip install pyarrow==15.0.2\n",
        "import pyarrow\n",
        "pyarrow.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "FrxQZpWiEQBh",
        "outputId": "5f49da74-aeca-47f4-ca01-4a2ee41144a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow==15.0.2\n",
            "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==15.0.2) (1.26.4)\n",
            "Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 17.0.0\n",
            "    Uninstalling pyarrow-17.0.0:\n",
            "      Successfully uninstalled pyarrow-17.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 15.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyarrow-15.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              },
              "id": "2b76d87f88644b07aa1b43395e47bac6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'14.0.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "import copy"
      ],
      "metadata": {
        "id": "GgWCORkjE-SR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import datasets\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "MA-mGoyaEWmV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "folder_path = '/content/DiscourseEE/Predictions-Results'\n",
        "folder = 'Event-Detection-Predictions'\n",
        "\n",
        "def read_pkl_file(name):\n",
        "    with open(name, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        return data\n",
        "\n",
        "file_name = os.path.join(folder_path, folder, 'event_detection_dataset.pkl')\n",
        "data = read_pkl_file(file_name)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db4Mm8CgEo3b",
        "outputId": "1fb271d5-81eb-48d9-9f6a-b978c1d0f842"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'taking-moud', 'relapse', 'tapering'],\n",
            "        num_rows: 213\n",
            "    })\n",
            "    val: Dataset({\n",
            "        features: ['text', 'taking-moud', 'relapse', 'tapering'],\n",
            "        num_rows: 48\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'taking-moud', 'relapse', 'tapering'],\n",
            "        num_rows: 99\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(data['test'])\n",
        "pprint(data['train'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIeiWy2AE62X",
        "outputId": "6eba6826-ba05-4f6d-9362-11b91739f663"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'taking-moud', 'relapse', 'tapering'],\n",
            "    num_rows: 99\n",
            "})\n",
            "{'relapse': 0,\n",
            " 'taking-moud': 0,\n",
            " 'tapering': 1,\n",
            " 'text': 'Do rapid suboxone tapers actually work? If so, what was your '\n",
            "         'experience and dosage?. Just looking to see what your experiences '\n",
            "         'were. Appreciate any info üôèüèº They acrually do work if you are on '\n",
            "         'heroin or oxy bit you gotta take it for 10 days max. You start with '\n",
            "         '8 mg and go down daily till 0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Previously trained model\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
        "\n",
        "def compute_metrics(pred, labels, threshold):\n",
        "    # Process the predictions and compute evaluation metrics\n",
        "    # Assuming the labels are in multihot format\n",
        "    # Convert the multihot encoded labels to binary\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(pred.predictions))\n",
        "    #print(probs)\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "    # print(\"y_pred\", y_pred)\n",
        "    # print(\"actual\", pred.label_ids)\n",
        "    y_true = pred.label_ids\n",
        "    #pprint(classification_report(y_true, y_pred, target_names = labels))\n",
        "    return precision_score(y_true, y_pred, average='macro'), recall_score(y_true, y_pred, average='macro'), f1_score(y_true, y_pred, average='macro')\n",
        "    # return {\n",
        "    #     #'classification_report': classification_report(binary_labels, binary_predictions, target_names=labels),\n",
        "    #     #'accuracy': accuracy_score(y_true, y_pred),\n",
        "    #     'precision': precision_score(y_true, y_pred, average='macro'),\n",
        "    #     'recall': recall_score(y_true, y_pred, average='macro'),\n",
        "    #     #'f1': f1_score(y_true, y_pred, average='weighted'),\n",
        "    #     'f1_macro': f1_score(y_true, y_pred, average='macro')\n",
        "    # }\n"
      ],
      "metadata": {
        "id": "quM241ReFOdc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "models = ['bert', 'mpnet', 'roberta']\n",
        "labels = ['taking-moud', 'relapse', 'tapering']\n",
        "numbers = ['1', '2', '3']\n",
        "\n",
        "for md_name in models:\n",
        "    pre, rec, f1_ls = [], [], []\n",
        "    for number in numbers:\n",
        "        file_name = os.path.join(folder_path, folder, f'{md_name}-predictions-{number}.pkl')\n",
        "        predictions = read_pkl_file(file_name)\n",
        "        p, r, f1 = compute_metrics(predictions, labels, 0.5)\n",
        "        # print(\"\\n----------------\")\n",
        "        # print(f'Model: {md_name}')\n",
        "        # print(f'Precision: {p}')\n",
        "        # print(f'Recall: {r}')\n",
        "        # print(f'F1: {f1}')\n",
        "        pre.append(p)\n",
        "        rec.append(r)\n",
        "        f1_ls.append(f1)\n",
        "\n",
        "    print(\"\\n************ [Mean and STD] **************************\")\n",
        "    print(md_name)\n",
        "    print(\"Precision: \",statistics.mean(pre)*100, statistics.stdev(pre)*100)\n",
        "    print(\"Recall: \",statistics.mean(rec)*100, statistics.stdev(rec)*100)\n",
        "    print(\"F1: \",statistics.mean(f1_ls)*100, statistics.stdev(f1_ls)*100)\n",
        "    print(\"*******************************************\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo5hXzOxFZLf",
        "outputId": "6002ffd2-7e73-46ae-9a60-04b01e5a457f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "************ [Mean and STD] **************************\n",
            "bert\n",
            "Precision:  48.8957138957139 0.3189929629912219\n",
            "Recall:  52.90360046457607 0.9523809523809545\n",
            "F1:  50.48115397957675 0.6453026404838998\n",
            "*******************************************\n",
            "\n",
            "************ [Mean and STD] **************************\n",
            "mpnet\n",
            "Precision:  47.91870048689866 1.184701837185945\n",
            "Recall:  65.88979223125564 2.073141210504757\n",
            "F1:  54.958634796802755 1.7688571223320817\n",
            "*******************************************\n",
            "\n",
            "************ [Mean and STD] **************************\n",
            "roberta\n",
            "Precision:  51.74436674436676 0.21532877172573497\n",
            "Recall:  59.59769002451929 2.2927488912940697\n",
            "F1:  55.26530571475524 1.227702853738046\n",
            "*******************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMs\n",
        "('gemma-1.1-7b-it', 'Mixtral-8x7B-Instruct-v0.1', 'Meta-Llama-3-8B-Instruct', 'Meta-Llama-3-70B-Instruct', 'gpt-4o-2024-05-13')\n"
      ],
      "metadata": {
        "id": "EnhwUoWbIVhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/DiscourseEE/Predictions-Results'\n",
        "folder = 'Event-Detection-Predictions'\n",
        "\n",
        "df_train = pd.read_csv(os.path.join(folder_path, folder, 'train.csv'))\n",
        "df_dev = pd.read_csv(os.path.join(folder_path, folder, 'dev.csv'))\n",
        "df_test = pd.read_csv(os.path.join(folder_path, folder, 'test.csv'))\n",
        "\n",
        "import ast\n",
        "\n",
        "def create_multi_label_data(df):\n",
        "\n",
        "    df['label'] = df['label'].apply(ast.literal_eval)\n",
        "\n",
        "    unique_labels = sorted(set(label for sublist in df_train['label'] for label in sublist))\n",
        "    print(unique_labels)\n",
        "    for label in unique_labels:\n",
        "        df[label] = df['label'].apply(lambda x: 1 if label in x else 0)\n",
        "    df['text'] = df['post'] + ' ' + df['comment']\n",
        "    df = df[['text', 'label']]\n",
        "    return df\n",
        "\n",
        "train = create_multi_label_data(df_train)\n",
        "val = create_multi_label_data(df_dev)\n",
        "test = create_multi_label_data(df_test)\n",
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "E7BtrwBjIe6P",
        "outputId": "8aef40b2-58a3-4bcc-ff49-0c35c8efa91f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['relapse', 'taking-moud', 'tapering']\n",
            "['relapse', 'taking-moud', 'tapering']\n",
            "['relapse', 'taking-moud', 'tapering']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text          label\n",
              "0  Do rapid suboxone tapers actually work? If so,...     [tapering]\n",
              "1  Can you be prescribed Z drugs or benzos while ...      [relapse]\n",
              "2  Powdering sub-lingual tablets?. Hello, Does an...  [taking-moud]\n",
              "3  This will probably get downvoted but here goes...      [relapse]\n",
              "4  Suboxone taper Doing a taper.. 8 mg to 4 mg to...     [tapering]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e64dee27-f133-4f37-af10-7275026b0fca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Do rapid suboxone tapers actually work? If so,...</td>\n",
              "      <td>[tapering]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Can you be prescribed Z drugs or benzos while ...</td>\n",
              "      <td>[relapse]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Powdering sub-lingual tablets?. Hello, Does an...</td>\n",
              "      <td>[taking-moud]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This will probably get downvoted but here goes...</td>\n",
              "      <td>[relapse]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Suboxone taper Doing a taper.. 8 mg to 4 mg to...</td>\n",
              "      <td>[tapering]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e64dee27-f133-4f37-af10-7275026b0fca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e64dee27-f133-4f37-af10-7275026b0fca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e64dee27-f133-4f37-af10-7275026b0fca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9f4c1653-08ea-4c38-a615-9bf37e5085ed\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9f4c1653-08ea-4c38-a615-9bf37e5085ed')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9f4c1653-08ea-4c38-a615-9bf37e5085ed button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 213,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 213,\n        \"samples\": [\n          \"Snorting suboxone to get off?. A friend of mine swears he got off subs by snorting them. He said it only works if it\\u2019s the subs with naloxone. His reasons seemed considerable. He said that he was able to quit after a month of snorting the 1/4 in the morning and 1/4 in the evening of the 8/2mg. Said when you snort the naloxone becomes active and so does the bupe. They kinda cancel each other out. You don\\u2019t feel bad but you don\\u2019t feel great and your body gets used to this. Almost like your not taking anything but not going into withdrawal. Has anyone else ever done this ? What are people\\u2019s thoughts ? Your friend may have successfully quit, but naloxone had nothing to do with it. I've snorted my subs plenty, and you get a higher bioavailability and catch a lil buzz, but the naloxone doesn't even apply. I think this may be a good way to taper when you get down below 0.5mg a day, but using 4mg intranasal is gonna cause some problems down the road. For one, sub is super acidic. Another issue is all that gummy junk in your sinuses.\",\n          \"Opioid detox in rehabs? What do you they do for there? Has anyone went away to a rehab for 2weeks- 1month for opiate dependency? What did they do to detox you. Did they taper you with subs for a week or two and you got off the subs before you left. Did you feel fine physically when you were done with the program. Or did you withdrawal from the suboxone afterwards when you left I was drug tested for the first few days until I had no fentanyl in my system and then they gave me a medicine called Subutex and tapered me off that and then then started the suboxone for maintenance\",\n          \"Suboxone and Total Knee Replacement. I currently take 8mg of Suboxone a day. Been on Suboxone for 10 years and 8 mg/day for 2 months. I'm having a TKR in two weeks. The surgeon and my Doc are aware and so will the anesthesiologist. I know this is a painful post-op, so wondering if anyone knows how to best manage this. I'm thinking of stopping the Subs one or two days before surgery, and then rely on whatever narcotic is prescribed. I think the narcotic should be a pretty strong dose to over some the Subs and not throw me into withdrawal. I'll leave it the the Drs to figure this out. Just wondering if anyone has any experience, suggestions. Thanks friends! Joe Just went thru something similar.. stopped day of, took pain medication eight days..waited 12 hrs and restarted subs, no problems\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json_file(name):\n",
        "    with open(name, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        return data"
      ],
      "metadata": {
        "id": "pSeMWupzIvlP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "#function for processing the raw predictions\n",
        "def processing_pred(dt):\n",
        "    all_classes = ['taking-moud', 'relapse', 'tapering']\n",
        "    predictions = []\n",
        "    for k,v in dt.items():\n",
        "        v = v.lower()\n",
        "        lst = []\n",
        "        for c in all_classes:\n",
        "            if c in v:\n",
        "              lst.append(c)\n",
        "        if(len(lst)==0):\n",
        "            lst = ['taking-moud']\n",
        "        # print(k, lst)\n",
        "        predictions.append(lst)\n",
        "    # print(predictions)\n",
        "    return predictions\n",
        "\n",
        "def result_print(actual_labels, predicted_labels):\n",
        "\n",
        "    # Convert string labels to binary format\n",
        "    all_classes = ['taking-moud', 'relapse', 'tapering']\n",
        "    mlb = MultiLabelBinarizer(classes= all_classes)\n",
        "    true_labels_encoded = mlb.fit_transform(actual_labels)\n",
        "    predicted_labels_encoded = mlb.transform(predicted_labels)\n",
        "\n",
        "\n",
        "    precision = precision_score(true_labels_encoded, predicted_labels_encoded, average= 'macro')\n",
        "    recall = recall_score(true_labels_encoded, predicted_labels_encoded, average='macro')\n",
        "    f1 = f1_score(true_labels_encoded, predicted_labels_encoded, average='weighted')\n",
        "    f1_macro = f1_score(true_labels_encoded, predicted_labels_encoded, average='macro')\n",
        "    #pprint(classification_report(true_labels_encoded, predicted_labels_encoded, target_names=all_classes))\n",
        "    # print(f'Precision: {precision}')\n",
        "    # print(f'Recall: {recall}')\n",
        "    # print(f'F1-score: {f1}')\n",
        "    # print(f'F1-score (macro): {f1_macro}')\n",
        "    return precision, recall, f1_macro"
      ],
      "metadata": {
        "id": "f8l_q-fhIxuJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_names = ['gemma-1.1-7b-it', 'Mixtral-8x7B-Instruct-v0.1', 'Meta-Llama-3-8B-Instruct', 'Meta-Llama-3-70B-Instruct', 'gpt-4o-2024-05-13']\n",
        "\n",
        "typ = 'zs' ##mention of prompt technique\n",
        "run = '3'\n",
        "runs = ['1', '2', '3']\n",
        "import statistics\n",
        "\n",
        "for i in range(len(mdl_names)):\n",
        "    p, r, f1 = [], [], []\n",
        "    for run in runs:\n",
        "        lm_model = mdl_names[i]\n",
        "        mdl_name = mdl_names[i]\n",
        "        nm = f'raw-{typ}-{mdl_name}-predictions-{run}.json'\n",
        "        file_name = os.path.join(folder_path, folder, nm)\n",
        "\n",
        "        raw_pred = read_json_file(file_name)\n",
        "        #print(raw_pred)\n",
        "        predicted_labels = processing_pred(raw_pred)\n",
        "        actual_labels = test['label']\n",
        "\n",
        "        # print(i, predicted_labels, actual_labels)\n",
        "        #print(\"\\n\\n\", mdl_name,\"\\n\")\n",
        "        pre, rec, f1_mac = result_print(actual_labels, predicted_labels)\n",
        "        p.append(pre)\n",
        "        r.append(rec)\n",
        "        f1.append(f1_mac)\n",
        "\n",
        "    print(\"\\n************** [Mean and STD] **************************\")\n",
        "    print(mdl_names[i])\n",
        "    print(\"Precision: \",statistics.mean(p)*100, statistics.stdev(p)*100)\n",
        "    print(\"Recall: \",statistics.mean(r)*100, statistics.stdev(r)*100)\n",
        "    print(\"F1: \",statistics.mean(f1)*100, statistics.stdev(f1)*100)\n",
        "    print(\"*******************************************\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf6SI1MJI0dk",
        "outputId": "cbb04b6e-84d7-4e3d-c5ca-52345600b89b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "************** [Mean and STD] **************************\n",
            "gemma-1.1-7b-it\n",
            "Precision:  54.9037999037999 1.3454843935660155\n",
            "Recall:  56.254032778423024 0.6867629508653146\n",
            "F1:  50.12901959951446 0.6667451751607549\n",
            "*******************************************\n",
            "\n",
            "************** [Mean and STD] **************************\n",
            "Mixtral-8x7B-Instruct-v0.1\n",
            "Precision:  55.094652411725576 0.0238133989346834\n",
            "Recall:  54.314750290360045 0.0\n",
            "F1:  51.757892790014836 0.04741368809463058\n",
            "*******************************************\n",
            "\n",
            "************** [Mean and STD] **************************\n",
            "Meta-Llama-3-8B-Instruct\n",
            "Precision:  60.621805095489314 0.09456049679402656\n",
            "Recall:  59.29442508710802 0.0\n",
            "F1:  55.42258840795537 0.032195468894390754\n",
            "*******************************************\n",
            "\n",
            "************** [Mean and STD] **************************\n",
            "Meta-Llama-3-70B-Instruct\n",
            "Precision:  61.21281464530893 0.0\n",
            "Recall:  62.388695315524586 0.0\n",
            "F1:  59.84380206793295 0.0\n",
            "*******************************************\n",
            "\n",
            "************** [Mean and STD] **************************\n",
            "gpt-4o-2024-05-13\n",
            "Precision:  62.36036241597978 1.1433697766726572\n",
            "Recall:  64.62608078461736 1.1773972916123592\n",
            "F1:  61.40942879499217 1.005211342797691\n",
            "*******************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instruction-tuned Models\n",
        "(FLAN-T5 base, FLAN-T5 large)"
      ],
      "metadata": {
        "id": "8aRhu1gUGEPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "\n",
        "def result_print(actual_labels, predicted_labels):\n",
        "# Convert string labels to binary format\n",
        "      all_classes = ['taking-moud', 'relapse', 'tapering']\n",
        "      mlb = MultiLabelBinarizer(classes= all_classes)\n",
        "      true_labels_encoded = mlb.fit_transform(actual_labels)\n",
        "      predicted_labels_encoded = mlb.transform(predicted_labels)\n",
        "\n",
        "      # for i in range(len(predicted_labels_encoded)):\n",
        "      #     print(predicted_labels_encoded[i], true_labels_encoded[i])\n",
        "\n",
        "      precision = precision_score(true_labels_encoded, predicted_labels_encoded, average= 'macro')\n",
        "      recall = recall_score(true_labels_encoded, predicted_labels_encoded, average='macro')\n",
        "      f1 = f1_score(true_labels_encoded, predicted_labels_encoded, average='weighted')\n",
        "      f1_macro = f1_score(true_labels_encoded, predicted_labels_encoded, average='macro')\n",
        "      #pprint(classification_report(true_labels_encoded, predicted_labels_encoded, target_names=all_classes))\n",
        "      # print(f'Precision: {precision}')\n",
        "      # print(f'Recall: {recall}')\n",
        "      # #print(f'F1-score: {f1}')\n",
        "      # print(f'F1-score (macro): {f1_macro}')\n",
        "      return precision, recall, f1_macro"
      ],
      "metadata": {
        "id": "9QKqjyLSFlBW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_names = ['flan-t5-base', 'flan-t5-large']\n",
        "runs = ['1', '2', '3']\n",
        "import statistics\n",
        "\n",
        "for i in range(len(mdl_names)):\n",
        "    p, r, f1 = [], [], []\n",
        "    for run in runs:\n",
        "        mdl_name = mdl_names[i]\n",
        "        nm = f'{mdl_name}-predictions-{run}.pkl'\n",
        "        file_name = os.path.join(folder_path, folder, nm)\n",
        "\n",
        "        predicted_labels = read_pkl_file(file_name)\n",
        "        actual_labels = test['label']\n",
        "\n",
        "        # print(predicted_labels)\n",
        "        # print(actual_labels)\n",
        "        #print(\"\\n\\n\", mdl_name,\"\\n\")\n",
        "        pre, rec, f1_mac = result_print(actual_labels, predicted_labels)\n",
        "        p.append(pre)\n",
        "        r.append(rec)\n",
        "        f1.append(f1_mac)\n",
        "\n",
        "    print(\"\\n***************[Mean and STD] *************************\")\n",
        "    print(mdl_names[i])\n",
        "    print(\"Precision: \",statistics.mean(p)*100, statistics.stdev(p)*100)\n",
        "    print(\"Recall: \",statistics.mean(r)*100, statistics.stdev(r)*100)\n",
        "    print(\"F1: \",statistics.mean(f1)*100, statistics.stdev(f1)*100)\n",
        "    print(\"*******************************************\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-xoMzFZGjiN",
        "outputId": "9a3d256b-071e-46fa-a725-2d0149115167"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "***************[Mean and STD] *************************\n",
            "flan-t5-base\n",
            "Precision:  54.21223040393494 3.109936991722142\n",
            "Recall:  53.48528842431281 3.7417419628703277\n",
            "F1:  51.26906598022829 0.5905704456785096\n",
            "*******************************************\n",
            "\n",
            "***************[Mean and STD] *************************\n",
            "flan-t5-large\n",
            "Precision:  57.617825304270184 1.1421019258016356\n",
            "Recall:  54.78190734288295 0.7736648952124889\n",
            "F1:  55.63246188322781 0.662984386339577\n",
            "*******************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Argument-Extraction-Results\n",
        "(0.749 means relaxed-match and 0.99 means exact match)\n"
      ],
      "metadata": {
        "id": "BEM68Wn_KLj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "def print_check_score(p_type, e_type, s_t, mdl_name):\n",
        "    root = '/content/DiscourseEE/Predictions-Results'\n",
        "    folder = 'Argument-Extraction-Results'\n",
        "\n",
        "    numbers = ['0', '1', '2']\n",
        "    event_types = ['taking-moud', 'relapse', 'tapering']\n",
        "    arg_types = ['main-arguments', 'event-specific-arguments', 'subject-effect-arguments']\n",
        "\n",
        "    all_results = {}\n",
        "    for number in numbers: #iterating over all 3 runs\n",
        "          file_name = f'{number}-new-results-{mdl_name}.json'\n",
        "          name = os.path.join(root, folder, file_name)\n",
        "          results = read_json_file(name)\n",
        "          data = results[f'{p_type}_{e_type}_{s_t}']\n",
        "\n",
        "          ls3 = []\n",
        "          for event in event_types:\n",
        "              ls1 = []\n",
        "              for arg_type in arg_types:\n",
        "                  arguments = list(data[event][arg_type].keys())\n",
        "                  ls = []\n",
        "                  for arg in arguments:\n",
        "                      f1_score = data[event][arg_type][arg][0][2] ##stored f1-score\n",
        "                      gt_count = data[event][arg_type][arg][1][2] ##stored ground-truth count\n",
        "                      if(gt_count>5):\n",
        "                          #print(event, arg_type, arg, \"{:.5f}\".format(f1_score))\n",
        "                          ls.append(f1_score)\n",
        "                  if(len(ls)>0):\n",
        "                     # print(event, arg_type, \"{:.4f}\".format(sum(ls)/len(ls)*100))\n",
        "\n",
        "                      key = f'{event}_{arg_type}'\n",
        "                      if key in all_results:\n",
        "                          all_results[key].append(sum(ls)/len(ls))\n",
        "                      else:\n",
        "                          all_results[key] = [sum(ls)/len(ls)]\n",
        "                      ls1.append(sum(ls)/len(ls))\n",
        "              if(len(ls1)>0):\n",
        "                  #print(\"{:.4f}\".format(sum(ls1)/len(ls1)*100,\"\\n\"))\n",
        "                  ls3.append(sum(ls1)/len(ls1))\n",
        "\n",
        "          key = f\"{p_type}_{e_type}_{s_t}\"\n",
        "          if key in all_results:\n",
        "              all_results[key].append(sum(ls3)/len(ls3))\n",
        "          else :\n",
        "              all_results[key] = [sum(ls3)/len(ls3)]\n",
        "\n",
        "          #print(\"\\nOverall\", \"{:.4f}\".format(sum(ls3)/len(ls3)*100))\n",
        "\n",
        "    print(\"********* Key Reuslts************\\n\")\n",
        "    for event in event_types:\n",
        "        for arg_type in arg_types:\n",
        "            key = f'{event}_{arg_type}'\n",
        "            value = all_results[key]\n",
        "            print(event, arg_type, \"[Mean:\",\"{:.4f}\".format(statistics.mean(value)*100),\n",
        "                  \"Std:\",\"{:.4f}\".format(statistics.stdev(value)*100),']')\n",
        "        print(\"\\n\")\n",
        "\n",
        "    key = f\"{p_type}_{e_type}_{s_t}\"\n",
        "    value = all_results[key]\n",
        "\n",
        "    print(\"Overall:\")\n",
        "    print(p_type, e_type, s_t, \"[Mean: \", \"{:.4f}\".format(statistics.mean(value)*100),\n",
        "      \"Std: \", \"{:.4f}\".format(statistics.stdev(value)*100), ']')\n",
        "    print(\"*************************************\\n\")\n",
        "    return all_results\n",
        "\n",
        "def read_json_file(name):\n",
        "    with open(name, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        return data\n",
        "\n"
      ],
      "metadata": {
        "id": "j2IyuB3BKywf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMs"
      ],
      "metadata": {
        "id": "x6iBzrCbN4YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_name = 'gemma-1.1-7b-it'\n",
        "prompt_types = ['batch']#['batch', 'isolated']\n",
        "exp_types = ['description_guided', 'question_guided']\n",
        "similarity_threshold = [0.749, 0.99]\n",
        "\n",
        "#0.749 indicate relaxed match and 0.99 indicates exact match\n",
        "\n",
        "results_for_print = {}\n",
        "for p_type in prompt_types:\n",
        "    for s_t in similarity_threshold:\n",
        "      for e_type in exp_types:\n",
        "              print(\"\\n-----------------------------------\")\n",
        "              print(p_type, e_type, s_t, mdl_name)\n",
        "              key = f'{p_type}_{e_type}_{s_t}'\n",
        "              value = print_check_score(p_type, e_type, s_t, mdl_name)\n",
        "              results_for_print[key] = value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCNXh6TQN4Bn",
        "outputId": "79a54a91-e750-40be-a7e5-ebfd00a0cbcc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------------------------------\n",
            "batch description_guided 0.749 gemma-1.1-7b-it\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 26.8418 Std: 1.0102 ]\n",
            "taking-moud event-specific-arguments [Mean: 39.1136 Std: 2.6050 ]\n",
            "taking-moud subject-effect-arguments [Mean: 31.8332 Std: 0.4294 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 20.4838 Std: 0.3202 ]\n",
            "relapse event-specific-arguments [Mean: 31.7544 Std: 0.6991 ]\n",
            "relapse subject-effect-arguments [Mean: 28.6655 Std: 4.5983 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 31.2400 Std: 2.5306 ]\n",
            "tapering event-specific-arguments [Mean: 28.6430 Std: 0.8509 ]\n",
            "tapering subject-effect-arguments [Mean: 30.2639 Std: 2.1919 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch description_guided 0.749 [Mean:  29.8710 Std:  0.3754 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "-----------------------------------\n",
            "batch question_guided 0.749 gemma-1.1-7b-it\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 25.5201 Std: 2.8904 ]\n",
            "taking-moud event-specific-arguments [Mean: 46.4686 Std: 0.2199 ]\n",
            "taking-moud subject-effect-arguments [Mean: 29.0011 Std: 3.6276 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 15.6695 Std: 0.1843 ]\n",
            "relapse event-specific-arguments [Mean: 33.5132 Std: 1.0004 ]\n",
            "relapse subject-effect-arguments [Mean: 24.9429 Std: 1.9885 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 27.6340 Std: 1.1901 ]\n",
            "tapering event-specific-arguments [Mean: 36.3903 Std: 0.5543 ]\n",
            "tapering subject-effect-arguments [Mean: 32.3867 Std: 3.5921 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch question_guided 0.749 [Mean:  30.1696 Std:  1.0353 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "-----------------------------------\n",
            "batch description_guided 0.99 gemma-1.1-7b-it\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 2.3584 Std: 2.2225 ]\n",
            "taking-moud event-specific-arguments [Mean: 20.8833 Std: 0.5383 ]\n",
            "taking-moud subject-effect-arguments [Mean: 14.2108 Std: 1.6908 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 1.1195 Std: 0.0146 ]\n",
            "relapse event-specific-arguments [Mean: 16.3324 Std: 0.2189 ]\n",
            "relapse subject-effect-arguments [Mean: 12.6560 Std: 3.1143 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 8.4504 Std: 0.5547 ]\n",
            "tapering event-specific-arguments [Mean: 17.7601 Std: 0.1636 ]\n",
            "tapering subject-effect-arguments [Mean: 12.5725 Std: 1.8952 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch description_guided 0.99 [Mean:  11.8159 Std:  0.2041 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "-----------------------------------\n",
            "batch question_guided 0.99 gemma-1.1-7b-it\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 3.4752 Std: 0.3634 ]\n",
            "taking-moud event-specific-arguments [Mean: 26.8540 Std: 0.3392 ]\n",
            "taking-moud subject-effect-arguments [Mean: 9.4843 Std: 0.4831 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 0.7042 Std: 1.2198 ]\n",
            "relapse event-specific-arguments [Mean: 19.7432 Std: 0.8581 ]\n",
            "relapse subject-effect-arguments [Mean: 14.0417 Std: 2.0906 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 1.7432 Std: 0.1015 ]\n",
            "tapering event-specific-arguments [Mean: 21.5231 Std: 2.5386 ]\n",
            "tapering subject-effect-arguments [Mean: 15.6625 Std: 1.0674 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch question_guided 0.99 [Mean:  12.5813 Std:  0.4378 ]\n",
            "*************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_name = 'Mixtral-8x7B-Instruct'\n",
        "prompt_types = ['batch']#['batch', 'isolated']\n",
        "exp_types = ['description_guided', 'question_guided']\n",
        "similarity_threshold = [0.749, 0.99]\n",
        "\n",
        "results_for_print = {}\n",
        "for p_type in prompt_types:\n",
        "    for s_t in similarity_threshold:\n",
        "      for e_type in exp_types:\n",
        "              print(\"\\n*************************\")\n",
        "              print(p_type, e_type, s_t, mdl_name)\n",
        "              key = f'{p_type}_{e_type}_{s_t}'\n",
        "              value = print_check_score(p_type, e_type, s_t, mdl_name)\n",
        "              results_for_print[key] = value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf6z2Ch7PyE1",
        "outputId": "8b46c698-776b-4fb9-e5d3-8febd3ab7e05"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************************\n",
            "batch description_guided 0.749 Mixtral-8x7B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 34.1959 Std: 0.3797 ]\n",
            "taking-moud event-specific-arguments [Mean: 30.7073 Std: 0.5402 ]\n",
            "taking-moud subject-effect-arguments [Mean: 33.9495 Std: 4.8293 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 33.7717 Std: 0.4582 ]\n",
            "relapse event-specific-arguments [Mean: 33.7356 Std: 0.9259 ]\n",
            "relapse subject-effect-arguments [Mean: 31.8006 Std: 0.7947 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 40.5556 Std: 0.1132 ]\n",
            "tapering event-specific-arguments [Mean: 41.5118 Std: 1.6082 ]\n",
            "tapering subject-effect-arguments [Mean: 39.4730 Std: 1.9771 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch description_guided 0.749 [Mean:  35.5223 Std:  0.7718 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch question_guided 0.749 Mixtral-8x7B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 36.8958 Std: 0.4886 ]\n",
            "taking-moud event-specific-arguments [Mean: 27.8890 Std: 0.4277 ]\n",
            "taking-moud subject-effect-arguments [Mean: 31.3199 Std: 1.0082 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 32.5361 Std: 1.5974 ]\n",
            "relapse event-specific-arguments [Mean: 33.0605 Std: 0.2997 ]\n",
            "relapse subject-effect-arguments [Mean: 20.4493 Std: 0.8115 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 33.6787 Std: 0.7413 ]\n",
            "tapering event-specific-arguments [Mean: 32.9515 Std: 1.0789 ]\n",
            "tapering subject-effect-arguments [Mean: 42.6990 Std: 0.6595 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch question_guided 0.749 [Mean:  32.3866 Std:  0.2406 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch description_guided 0.99 Mixtral-8x7B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 7.2543 Std: 0.4968 ]\n",
            "taking-moud event-specific-arguments [Mean: 18.3441 Std: 0.4727 ]\n",
            "taking-moud subject-effect-arguments [Mean: 11.4127 Std: 1.4126 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 3.8037 Std: 0.0000 ]\n",
            "relapse event-specific-arguments [Mean: 13.1247 Std: 0.2999 ]\n",
            "relapse subject-effect-arguments [Mean: 12.1743 Std: 0.8268 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 8.7963 Std: 0.8019 ]\n",
            "tapering event-specific-arguments [Mean: 19.0170 Std: 0.1673 ]\n",
            "tapering subject-effect-arguments [Mean: 11.6819 Std: 0.0238 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch description_guided 0.99 [Mean:  11.7343 Std:  0.2554 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch question_guided 0.99 Mixtral-8x7B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 6.1711 Std: 0.4609 ]\n",
            "taking-moud event-specific-arguments [Mean: 15.9411 Std: 0.4277 ]\n",
            "taking-moud subject-effect-arguments [Mean: 9.5990 Std: 0.0909 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 1.5600 Std: 0.4955 ]\n",
            "relapse event-specific-arguments [Mean: 12.7464 Std: 0.3112 ]\n",
            "relapse subject-effect-arguments [Mean: 10.8625 Std: 0.1470 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 7.7143 Std: 0.4949 ]\n",
            "tapering event-specific-arguments [Mean: 17.3919 Std: 0.3477 ]\n",
            "tapering subject-effect-arguments [Mean: 16.4632 Std: 0.0156 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch question_guided 0.99 [Mean:  10.9388 Std:  0.1331 ]\n",
            "*************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_name = 'Llama-3-8B-Instruct'\n",
        "prompt_types = ['batch']#['batch', 'isolated']\n",
        "exp_types = ['description_guided', 'question_guided']\n",
        "similarity_threshold = [0.749, 0.99]\n",
        "\n",
        "results_for_print = {}\n",
        "for p_type in prompt_types:\n",
        "    for s_t in similarity_threshold:\n",
        "      for e_type in exp_types:\n",
        "              print(\"\\n*************************\")\n",
        "              print(p_type, e_type, s_t, mdl_name)\n",
        "              key = f'{p_type}_{e_type}_{s_t}'\n",
        "              value = print_check_score(p_type, e_type, s_t, mdl_name)\n",
        "              results_for_print[key] = value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pod-bU1BP6Q_",
        "outputId": "12d3490a-033d-4793-a654-5153d737348c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************************\n",
            "batch description_guided 0.749 Llama-3-8B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 32.8867 Std: 0.4818 ]\n",
            "taking-moud event-specific-arguments [Mean: 48.4557 Std: 0.4532 ]\n",
            "taking-moud subject-effect-arguments [Mean: 32.4810 Std: 2.0809 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 33.4360 Std: 0.1239 ]\n",
            "relapse event-specific-arguments [Mean: 37.7479 Std: 0.0000 ]\n",
            "relapse subject-effect-arguments [Mean: 27.4940 Std: 0.0000 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 41.3364 Std: 0.7128 ]\n",
            "tapering event-specific-arguments [Mean: 42.8850 Std: 0.0000 ]\n",
            "tapering subject-effect-arguments [Mean: 35.5477 Std: 0.0000 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch description_guided 0.749 [Mean:  36.9189 Std:  0.2921 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch question_guided 0.749 Llama-3-8B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 34.8855 Std: 0.4264 ]\n",
            "taking-moud event-specific-arguments [Mean: 42.1992 Std: 0.7698 ]\n",
            "taking-moud subject-effect-arguments [Mean: 27.6477 Std: 0.2578 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 33.3109 Std: 0.4485 ]\n",
            "relapse event-specific-arguments [Mean: 32.3816 Std: 0.6873 ]\n",
            "relapse subject-effect-arguments [Mean: 31.4896 Std: 0.0418 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 25.8238 Std: 0.7587 ]\n",
            "tapering event-specific-arguments [Mean: 45.8903 Std: 0.0000 ]\n",
            "tapering subject-effect-arguments [Mean: 41.0900 Std: 0.0000 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch question_guided 0.749 [Mean:  34.9687 Std:  0.0736 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch description_guided 0.99 Llama-3-8B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 2.3444 Std: 0.4524 ]\n",
            "taking-moud event-specific-arguments [Mean: 28.8898 Std: 0.1322 ]\n",
            "taking-moud subject-effect-arguments [Mean: 9.7947 Std: 0.9360 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 4.4258 Std: 0.0000 ]\n",
            "relapse event-specific-arguments [Mean: 19.0503 Std: 0.6873 ]\n",
            "relapse subject-effect-arguments [Mean: 13.8689 Std: 0.0000 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 8.1982 Std: 0.0000 ]\n",
            "tapering event-specific-arguments [Mean: 25.8856 Std: 0.0000 ]\n",
            "tapering subject-effect-arguments [Mean: 15.2418 Std: 0.0000 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch description_guided 0.99 [Mean:  14.1888 Std:  0.1452 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch question_guided 0.99 Llama-3-8B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 1.8592 Std: 0.0153 ]\n",
            "taking-moud event-specific-arguments [Mean: 21.7269 Std: 0.7698 ]\n",
            "taking-moud subject-effect-arguments [Mean: 6.9949 Std: 0.0189 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 4.9249 Std: 0.0000 ]\n",
            "relapse event-specific-arguments [Mean: 18.8858 Std: 0.0000 ]\n",
            "relapse subject-effect-arguments [Mean: 11.7774 Std: 0.0157 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 5.8130 Std: 0.0342 ]\n",
            "tapering event-specific-arguments [Mean: 25.7523 Std: 0.0000 ]\n",
            "tapering subject-effect-arguments [Mean: 14.3170 Std: 0.0000 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch question_guided 0.99 [Mean:  12.4502 Std:  0.0906 ]\n",
            "*************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_name = 'Llama-3-70B-Instruct'\n",
        "prompt_types = ['batch']#['batch', 'isolated']\n",
        "exp_types = ['description_guided', 'question_guided']\n",
        "similarity_threshold = [0.749, 0.99]\n",
        "\n",
        "results_for_print = {}\n",
        "for p_type in prompt_types:\n",
        "    for s_t in similarity_threshold:\n",
        "      for e_type in exp_types:\n",
        "              print(\"\\n*************************\")\n",
        "              print(p_type, e_type, s_t, mdl_name)\n",
        "              key = f'{p_type}_{e_type}_{s_t}'\n",
        "              value = print_check_score(p_type, e_type, s_t, mdl_name)\n",
        "              results_for_print[key] = value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ35JbBFP9z_",
        "outputId": "a372b91f-91ef-455e-c41a-fe0ac7a280ec"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************************\n",
            "batch description_guided 0.749 Llama-3-70B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 41.3565 Std: 3.4753 ]\n",
            "taking-moud event-specific-arguments [Mean: 39.3901 Std: 1.0344 ]\n",
            "taking-moud subject-effect-arguments [Mean: 25.2899 Std: 2.3213 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 35.2064 Std: 2.8816 ]\n",
            "relapse event-specific-arguments [Mean: 38.8045 Std: 4.2920 ]\n",
            "relapse subject-effect-arguments [Mean: 30.7875 Std: 4.2407 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 41.5460 Std: 3.6548 ]\n",
            "tapering event-specific-arguments [Mean: 40.5723 Std: 0.4514 ]\n",
            "tapering subject-effect-arguments [Mean: 28.8311 Std: 4.1047 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch description_guided 0.749 [Mean:  35.7538 Std:  0.8714 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch question_guided 0.749 Llama-3-70B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 37.2819 Std: 0.6907 ]\n",
            "taking-moud event-specific-arguments [Mean: 42.3178 Std: 0.7354 ]\n",
            "taking-moud subject-effect-arguments [Mean: 25.0271 Std: 2.1089 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 35.9497 Std: 3.9304 ]\n",
            "relapse event-specific-arguments [Mean: 41.4241 Std: 5.1298 ]\n",
            "relapse subject-effect-arguments [Mean: 33.4937 Std: 1.6252 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 36.1320 Std: 0.7120 ]\n",
            "tapering event-specific-arguments [Mean: 40.2828 Std: 0.3159 ]\n",
            "tapering subject-effect-arguments [Mean: 34.9028 Std: 2.6368 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch question_guided 0.749 [Mean:  36.3124 Std:  0.8948 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch description_guided 0.99 Llama-3-70B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 6.7545 Std: 1.8900 ]\n",
            "taking-moud event-specific-arguments [Mean: 25.3907 Std: 0.8543 ]\n",
            "taking-moud subject-effect-arguments [Mean: 9.7004 Std: 0.9032 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 5.6379 Std: 0.0516 ]\n",
            "relapse event-specific-arguments [Mean: 21.0433 Std: 2.7505 ]\n",
            "relapse subject-effect-arguments [Mean: 13.1791 Std: 1.4016 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 11.0966 Std: 1.6484 ]\n",
            "tapering event-specific-arguments [Mean: 22.1431 Std: 0.1070 ]\n",
            "tapering subject-effect-arguments [Mean: 15.3041 Std: 0.2123 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch description_guided 0.99 [Mean:  14.4722 Std:  0.4416 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch question_guided 0.99 Llama-3-70B-Instruct\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 3.3350 Std: 1.5765 ]\n",
            "taking-moud event-specific-arguments [Mean: 25.8435 Std: 1.2871 ]\n",
            "taking-moud subject-effect-arguments [Mean: 10.2197 Std: 1.6176 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 5.8249 Std: 0.6270 ]\n",
            "relapse event-specific-arguments [Mean: 23.3383 Std: 2.5633 ]\n",
            "relapse subject-effect-arguments [Mean: 14.6212 Std: 0.4242 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 12.1153 Std: 2.1869 ]\n",
            "tapering event-specific-arguments [Mean: 23.0280 Std: 0.8673 ]\n",
            "tapering subject-effect-arguments [Mean: 18.5613 Std: 0.0616 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch question_guided 0.99 [Mean:  15.2097 Std:  0.1856 ]\n",
            "*************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_name = 'gpt-4o'\n",
        "prompt_types = ['batch']#['batch', 'isolated']\n",
        "exp_types = ['description_guided', 'question_guided']\n",
        "similarity_threshold = [0.749, 0.99]\n",
        "\n",
        "results_for_print = {}\n",
        "for p_type in prompt_types:\n",
        "    for s_t in similarity_threshold:\n",
        "      for e_type in exp_types:\n",
        "              print(\"\\n*************************\")\n",
        "              print(p_type, e_type, s_t, mdl_name)\n",
        "              key = f'{p_type}_{e_type}_{s_t}'\n",
        "              value = print_check_score(p_type, e_type, s_t, mdl_name)\n",
        "              results_for_print[key] = value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-6iZa5_QBaz",
        "outputId": "6581bfa9-07b1-46ab-e736-6ab971f95d5a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************************\n",
            "batch description_guided 0.749 gpt-4o\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 37.8800 Std: 2.0054 ]\n",
            "taking-moud event-specific-arguments [Mean: 46.3411 Std: 1.0525 ]\n",
            "taking-moud subject-effect-arguments [Mean: 30.5091 Std: 1.1099 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 43.5650 Std: 1.3968 ]\n",
            "relapse event-specific-arguments [Mean: 41.9453 Std: 2.1800 ]\n",
            "relapse subject-effect-arguments [Mean: 39.6814 Std: 2.3105 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 42.9098 Std: 1.1140 ]\n",
            "tapering event-specific-arguments [Mean: 38.4389 Std: 1.9056 ]\n",
            "tapering subject-effect-arguments [Mean: 43.1596 Std: 2.6988 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch description_guided 0.749 [Mean:  40.4922 Std:  0.2273 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch question_guided 0.749 gpt-4o\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 35.7777 Std: 1.0114 ]\n",
            "taking-moud event-specific-arguments [Mean: 47.3871 Std: 0.6394 ]\n",
            "taking-moud subject-effect-arguments [Mean: 38.8377 Std: 2.7906 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 39.7528 Std: 4.3803 ]\n",
            "relapse event-specific-arguments [Mean: 40.4128 Std: 0.2453 ]\n",
            "relapse subject-effect-arguments [Mean: 49.3598 Std: 1.0428 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 40.6981 Std: 1.5616 ]\n",
            "tapering event-specific-arguments [Mean: 44.4110 Std: 2.3900 ]\n",
            "tapering subject-effect-arguments [Mean: 41.2692 Std: 3.4805 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch question_guided 0.749 [Mean:  41.9896 Std:  0.8780 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch description_guided 0.99 gpt-4o\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 2.3616 Std: 0.9791 ]\n",
            "taking-moud event-specific-arguments [Mean: 27.3015 Std: 1.0856 ]\n",
            "taking-moud subject-effect-arguments [Mean: 14.7971 Std: 0.9979 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 7.3928 Std: 0.7446 ]\n",
            "relapse event-specific-arguments [Mean: 23.0470 Std: 1.8008 ]\n",
            "relapse subject-effect-arguments [Mean: 21.0440 Std: 2.1779 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 8.4296 Std: 0.0890 ]\n",
            "tapering event-specific-arguments [Mean: 20.3575 Std: 0.7347 ]\n",
            "tapering subject-effect-arguments [Mean: 16.7774 Std: 1.8649 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch description_guided 0.99 [Mean:  15.7232 Std:  0.2616 ]\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "batch question_guided 0.99 gpt-4o\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 4.9584 Std: 1.4543 ]\n",
            "taking-moud event-specific-arguments [Mean: 29.0657 Std: 0.2128 ]\n",
            "taking-moud subject-effect-arguments [Mean: 19.3587 Std: 0.7905 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 6.2240 Std: 1.3527 ]\n",
            "relapse event-specific-arguments [Mean: 21.1936 Std: 1.7692 ]\n",
            "relapse subject-effect-arguments [Mean: 26.1068 Std: 0.8416 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 8.7278 Std: 0.3527 ]\n",
            "tapering event-specific-arguments [Mean: 21.0687 Std: 2.1978 ]\n",
            "tapering subject-effect-arguments [Mean: 14.8607 Std: 0.0715 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "batch question_guided 0.99 [Mean:  16.8405 Std:  0.2343 ]\n",
            "*************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Baseline Models"
      ],
      "metadata": {
        "id": "3mitNf4ZQT16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "def print_check_score_2(s_t, mdl_name):\n",
        "    root = '/content/DiscourseEE/Predictions-Results'\n",
        "    folder = 'Argument-Extraction-Results'\n",
        "\n",
        "    numbers = ['0', '1', '2']\n",
        "    event_types = ['taking-moud', 'relapse', 'tapering']\n",
        "    arg_types = ['main-arguments', 'event-specific-arguments', 'subject-effect-arguments']\n",
        "\n",
        "    all_results = {}\n",
        "    for number in numbers: #iterating over all 3 runs\n",
        "          file_name = f'{number}-new-results-{mdl_name}.json'\n",
        "          name = os.path.join(root, folder, file_name)\n",
        "          results = read_json_file(name)\n",
        "          data = results[f'{number}-{mdl_name}-{s_t}']\n",
        "          #pprint(data)\n",
        "          ls3 = []\n",
        "          for event in event_types:\n",
        "              ls1 = []\n",
        "              for arg_type in arg_types:\n",
        "                  arguments = list(data[event][arg_type].keys())\n",
        "                  ls = []\n",
        "                  for arg in arguments:\n",
        "                      f1_score = data[event][arg_type][arg][0][2] ##stored f1-score\n",
        "                      gt_count = data[event][arg_type][arg][1][2] ##stored ground-truth count\n",
        "                      if(gt_count>5):\n",
        "                          #print(event, arg_type, arg, \"{:.5f}\".format(f1_score))\n",
        "                          ls.append(f1_score)\n",
        "                  if(len(ls)>0):\n",
        "                     # print(event, arg_type, \"{:.4f}\".format(sum(ls)/len(ls)*100))\n",
        "\n",
        "                      key = f'{event}_{arg_type}'\n",
        "                      if key in all_results:\n",
        "                          all_results[key].append(sum(ls)/len(ls))\n",
        "                      else:\n",
        "                          all_results[key] = [sum(ls)/len(ls)]\n",
        "                      ls1.append(sum(ls)/len(ls))\n",
        "              if(len(ls1)>0):\n",
        "                  #print(\"{:.4f}\".format(sum(ls1)/len(ls1)*100,\"\\n\"))\n",
        "                  ls3.append(sum(ls1)/len(ls1))\n",
        "\n",
        "          key = f\"{mdl_name}-{s_t}\"\n",
        "          if key in all_results:\n",
        "              all_results[key].append(sum(ls3)/len(ls3))\n",
        "          else :\n",
        "              all_results[key] = [sum(ls3)/len(ls3)]\n",
        "\n",
        "          #print(\"\\nOverall\", \"{:.4f}\".format(sum(ls3)/len(ls3)*100))\n",
        "\n",
        "    print(\"********* Key Reuslts************\\n\")\n",
        "    for event in event_types:\n",
        "        for arg_type in arg_types:\n",
        "            key = f'{event}_{arg_type}'\n",
        "            value = all_results[key]\n",
        "            print(event, arg_type, \"[Mean:\", \"{:.4f}\".format(statistics.mean(value)*100),\n",
        "                  \"Std: \", \"{:.4f}\".format(statistics.stdev(value)*100),']')\n",
        "        print('\\n')\n",
        "\n",
        "    key = f\"{mdl_name}-{s_t}\"\n",
        "    value = all_results[key]\n",
        "\n",
        "    print(\"Overall:\")\n",
        "    print(mdl_name, s_t, \"Mean:\", \"{:.4f}\".format(statistics.mean(value)*100),\n",
        "      \"Std:\", \"{:.4f}\".format(statistics.stdev(value)*100))\n",
        "    print(\"*************************************\\n\")\n",
        "    return all_results\n",
        "\n"
      ],
      "metadata": {
        "id": "lrKbY3ccQXJB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_name = 'extractive_qa_fine_tuned_squad'\n",
        "similarity_threshold = [0.749, 0.99]\n",
        "results_for_print = {}\n",
        "\n",
        "for s_t in similarity_threshold:\n",
        "          print(\"\\n*************************\")\n",
        "          print(s_t, mdl_name)\n",
        "          key = f'{mdl_name}_{s_t}'\n",
        "          value = print_check_score_2(s_t, mdl_name)\n",
        "          results_for_print[key] = value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLtVyjPlRHcl",
        "outputId": "cbeab4f1-a4b1-46e7-c7fa-607388a88e58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************************\n",
            "0.749 extractive_qa_fine_tuned_squad\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 4.9856 Std:  0.0000 ]\n",
            "taking-moud event-specific-arguments [Mean: 19.2658 Std:  0.0000 ]\n",
            "taking-moud subject-effect-arguments [Mean: 16.0842 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 14.9505 Std:  0.0000 ]\n",
            "relapse event-specific-arguments [Mean: 29.1571 Std:  0.0000 ]\n",
            "relapse subject-effect-arguments [Mean: 15.3909 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 19.9813 Std:  0.0000 ]\n",
            "tapering event-specific-arguments [Mean: 18.6588 Std:  0.0000 ]\n",
            "tapering subject-effect-arguments [Mean: 15.7460 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "extractive_qa_fine_tuned_squad 0.749 Mean: 17.1356 Std: 0.0000\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "0.99 extractive_qa_fine_tuned_squad\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 1.2579 Std:  0.0000 ]\n",
            "taking-moud event-specific-arguments [Mean: 7.8374 Std:  0.0000 ]\n",
            "taking-moud subject-effect-arguments [Mean: 1.5094 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 3.3621 Std:  0.0000 ]\n",
            "relapse event-specific-arguments [Mean: 14.8549 Std:  0.0000 ]\n",
            "relapse subject-effect-arguments [Mean: 6.4722 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 5.1167 Std:  0.0000 ]\n",
            "tapering event-specific-arguments [Mean: 4.2028 Std:  0.0000 ]\n",
            "tapering subject-effect-arguments [Mean: 4.0000 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "extractive_qa_fine_tuned_squad 0.99 Mean: 5.4015 Std: 0.0000\n",
            "*************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_name = 'generative-qa-flan-t5-base'\n",
        "similarity_threshold = [0.749, 0.99]\n",
        "results_for_print = {}\n",
        "\n",
        "for s_t in similarity_threshold:\n",
        "          print(\"\\n*************************\")\n",
        "          print(s_t, mdl_name)\n",
        "          key = f'{mdl_name}_{s_t}'\n",
        "          value = print_check_score_2(s_t, mdl_name)\n",
        "          results_for_print[key] = value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FMBxnGTRK68",
        "outputId": "c37c3bac-cb6d-4b00-8452-bd23482aa6f9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************************\n",
            "0.749 generative-qa-flan-t5-base\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 34.9931 Std:  0.0000 ]\n",
            "taking-moud event-specific-arguments [Mean: 37.2296 Std:  0.0000 ]\n",
            "taking-moud subject-effect-arguments [Mean: 11.4086 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 25.3799 Std:  0.0000 ]\n",
            "relapse event-specific-arguments [Mean: 20.3118 Std:  0.0000 ]\n",
            "relapse subject-effect-arguments [Mean: 11.3780 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 38.7827 Std:  0.0000 ]\n",
            "tapering event-specific-arguments [Mean: 40.9407 Std:  0.0000 ]\n",
            "tapering subject-effect-arguments [Mean: 17.5392 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "generative-qa-flan-t5-base 0.749 Mean: 26.4404 Std: 0.0000\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "0.99 generative-qa-flan-t5-base\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 4.8934 Std:  0.0000 ]\n",
            "taking-moud event-specific-arguments [Mean: 25.2465 Std:  0.0000 ]\n",
            "taking-moud subject-effect-arguments [Mean: 2.8533 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 3.7518 Std:  0.0000 ]\n",
            "relapse event-specific-arguments [Mean: 13.3183 Std:  0.0000 ]\n",
            "relapse subject-effect-arguments [Mean: 11.3780 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 7.4074 Std:  0.0000 ]\n",
            "tapering event-specific-arguments [Mean: 21.9303 Std:  0.0000 ]\n",
            "tapering subject-effect-arguments [Mean: 6.6765 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "generative-qa-flan-t5-base 0.99 Mean: 10.8284 Std: 0.0000\n",
            "*************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_name = 'generative-qa-flan-t5-large'\n",
        "similarity_threshold = [0.749, 0.99]\n",
        "results_for_print = {}\n",
        "\n",
        "for s_t in similarity_threshold:\n",
        "          print(\"\\n*************************\")\n",
        "          print(s_t, mdl_name)\n",
        "          key = f'{mdl_name}_{s_t}'\n",
        "          value = print_check_score_2(s_t, mdl_name)\n",
        "          results_for_print[key] = value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcFem3xsReY6",
        "outputId": "7fa18f7a-d9a8-41d4-9a13-a285ea1ad8ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************************\n",
            "0.749 generative-qa-flan-t5-large\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 41.7027 Std:  0.0000 ]\n",
            "taking-moud event-specific-arguments [Mean: 45.6181 Std:  0.0000 ]\n",
            "taking-moud subject-effect-arguments [Mean: 23.9258 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 38.4419 Std:  0.0000 ]\n",
            "relapse event-specific-arguments [Mean: 27.4133 Std:  0.0000 ]\n",
            "relapse subject-effect-arguments [Mean: 19.7963 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 44.0440 Std:  0.0000 ]\n",
            "tapering event-specific-arguments [Mean: 51.9250 Std:  0.0000 ]\n",
            "tapering subject-effect-arguments [Mean: 26.9329 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "generative-qa-flan-t5-large 0.749 Mean: 35.5333 Std: 0.0000\n",
            "*************************************\n",
            "\n",
            "\n",
            "*************************\n",
            "0.99 generative-qa-flan-t5-large\n",
            "********* Key Reuslts************\n",
            "\n",
            "taking-moud main-arguments [Mean: 5.1082 Std:  0.0000 ]\n",
            "taking-moud event-specific-arguments [Mean: 32.3349 Std:  0.0000 ]\n",
            "taking-moud subject-effect-arguments [Mean: 7.8731 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "relapse main-arguments [Mean: 7.0914 Std:  0.0000 ]\n",
            "relapse event-specific-arguments [Mean: 18.5748 Std:  0.0000 ]\n",
            "relapse subject-effect-arguments [Mean: 14.6620 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "tapering main-arguments [Mean: 10.0672 Std:  0.0000 ]\n",
            "tapering event-specific-arguments [Mean: 33.5805 Std:  0.0000 ]\n",
            "tapering subject-effect-arguments [Mean: 10.8117 Std:  0.0000 ]\n",
            "\n",
            "\n",
            "Overall:\n",
            "generative-qa-flan-t5-large 0.99 Mean: 15.5671 Std: 0.0000\n",
            "*************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explicit-Implicit-Scattered Argument Results"
      ],
      "metadata": {
        "id": "IUuTX0i2Szmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##for similarity threshold 0.749 and 0.99\n",
        "numbers = ['0', '1', '2']\n",
        "folder_path = '/content/DiscourseEE/Predictions-Results/Argument-Extraction-Results'\n",
        "names = ['gemma-1.1-7b-it', 'Mixtral-8x7B-Instruct', 'Llama-3-8B-Instruct',\n",
        "         'Llama-3-70B-Instruct', 'gpt-4o']"
      ],
      "metadata": {
        "id": "41j66GkWSzXE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "numbers = ['0', '1', '2']\n",
        "e_types = ['rm', 'em']\n",
        "guides = ['description-guided', 'question-guided']\n",
        "## rm indicate relaxed-match and em indicates exact-match\n",
        "\n",
        "for e_type in e_types:\n",
        "    for guide in guides:\n",
        "        for name in names:\n",
        "            #print(name)\n",
        "            nm = f'eis-results-{name}.json'\n",
        "            results_data_dict = read_json_file(os.path.join(folder_path, nm))\n",
        "            #pprint(results_data_dict)\n",
        "            data_dict = results_data_dict[f'{name}-{guide}']\n",
        "                #print(data_dict)\n",
        "\n",
        "            explicit_lst, implicit_lst, scattered_lst = [], [], []\n",
        "\n",
        "            print(\"\\n******************************\")\n",
        "            print(name, guide, e_type)\n",
        "            for number in numbers:\n",
        "                explicit_lst.append(data_dict[f'{number}-explicit-{e_type}'])\n",
        "                implicit_lst.append(data_dict[f'{number}-implicit-{e_type}'])\n",
        "                scattered_lst.append(data_dict[f'{number}-scattered-{e_type}'])\n",
        "\n",
        "            print(\"----------------\")\n",
        "            print(\"Explicit: \", statistics.mean(explicit_lst)*100, statistics.stdev(explicit_lst)*100)\n",
        "            print(\"Implicit: \", statistics.mean(implicit_lst)*100, statistics.stdev(implicit_lst)*100)\n",
        "            print(\"Scattered: \", statistics.mean(scattered_lst)*100, statistics.stdev(scattered_lst)*100)\n",
        "        print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40ywdSluRhBV",
        "outputId": "e76f0638-75f3-4937-8684-c7e18b7db3af"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "******************************\n",
            "gemma-1.1-7b-it description-guided rm\n",
            "----------------\n",
            "Explicit:  37.96296296296296 0.534583582582986\n",
            "Implicit:  24.13087934560327 0.7084052382694802\n",
            "Scattered:  28.31541218637993 0.6208067410641164\n",
            "\n",
            "******************************\n",
            "Mixtral-8x7B-Instruct description-guided rm\n",
            "----------------\n",
            "Explicit:  52.2633744855967 0.4714584048308499\n",
            "Implicit:  27.539195637355146 0.4256985683979805\n",
            "Scattered:  48.2078853046595 0.3104033705320566\n",
            "\n",
            "******************************\n",
            "Llama-3-8B-Instruct description-guided rm\n",
            "----------------\n",
            "Explicit:  46.09053497942387 0.17819452752766304\n",
            "Implicit:  31.765507839127473 0.11806753971158031\n",
            "Scattered:  38.88888888888889 0.6208067410641132\n",
            "\n",
            "******************************\n",
            "Llama-3-70B-Instruct description-guided rm\n",
            "----------------\n",
            "Explicit:  52.67489711934156 2.316528857859607\n",
            "Implicit:  26.993865030674847 0.0\n",
            "Scattered:  43.36917562724014 1.3530169238836458\n",
            "\n",
            "******************************\n",
            "gpt-4o description-guided rm\n",
            "----------------\n",
            "Explicit:  53.39506172839506 0.0\n",
            "Implicit:  33.4696659850034 0.3123773479860842\n",
            "Scattered:  54.121863799283155 1.119175268530178\n",
            "\n",
            "\n",
            "\n",
            "******************************\n",
            "gemma-1.1-7b-it question-guided rm\n",
            "----------------\n",
            "Explicit:  40.02057613168725 1.4255562202212981\n",
            "Implicit:  25.01704158145876 1.6529455559621191\n",
            "Scattered:  27.77777777777778 1.2416134821282296\n",
            "\n",
            "******************************\n",
            "Mixtral-8x7B-Instruct question-guided rm\n",
            "----------------\n",
            "Explicit:  47.8395061728395 0.8165899108224051\n",
            "Implicit:  26.857532379004773 0.3123773479860842\n",
            "Scattered:  50.53763440860215 1.0752688172043001\n",
            "\n",
            "******************************\n",
            "Llama-3-8B-Instruct question-guided rm\n",
            "----------------\n",
            "Explicit:  41.358024691358025 0.0\n",
            "Implicit:  30.67484662576687 0.0\n",
            "Scattered:  36.91756272401434 0.3104033705320566\n",
            "\n",
            "******************************\n",
            "Llama-3-70B-Instruct question-guided rm\n",
            "----------------\n",
            "Explicit:  53.49794238683128 3.0293069679702596\n",
            "Implicit:  28.3571915473756 0.6247546959721659\n",
            "Scattered:  41.21863799283154 4.508869399788216\n",
            "\n",
            "******************************\n",
            "gpt-4o question-guided rm\n",
            "----------------\n",
            "Explicit:  55.144032921810705 1.86040548559929\n",
            "Implicit:  36.5371506475801 0.1180675397115771\n",
            "Scattered:  49.82078853046595 2.7589255050789707\n",
            "\n",
            "\n",
            "\n",
            "******************************\n",
            "gemma-1.1-7b-it description-guided em\n",
            "----------------\n",
            "Explicit:  26.74897119341564 0.890972637638312\n",
            "Implicit:  7.430129516019086 0.11806753971157949\n",
            "Scattered:  0.896057347670251 0.3104033705320569\n",
            "\n",
            "******************************\n",
            "Mixtral-8x7B-Instruct description-guided em\n",
            "----------------\n",
            "Explicit:  34.773662551440324 0.17819452752766304\n",
            "Implicit:  6.5439672801636 0.20449897750511245\n",
            "Scattered:  2.867383512544803 0.3104033705320568\n",
            "\n",
            "******************************\n",
            "Llama-3-8B-Instruct description-guided em\n",
            "----------------\n",
            "Explicit:  33.33333333333333 0.30864197530864057\n",
            "Implicit:  10.224948875255624 0.0\n",
            "Scattered:  1.0752688172043012 0.0\n",
            "\n",
            "******************************\n",
            "Llama-3-70B-Instruct description-guided em\n",
            "----------------\n",
            "Explicit:  40.02057613168724 2.588425025804349\n",
            "Implicit:  6.952965235173824 0.3542026191347401\n",
            "Scattered:  3.9426523297491043 0.6208067410641139\n",
            "\n",
            "******************************\n",
            "gpt-4o description-guided em\n",
            "----------------\n",
            "Explicit:  37.96296296296296 0.9259259259259273\n",
            "Implicit:  10.770279481935924 0.11806753971157949\n",
            "Scattered:  3.225806451612903 0.0\n",
            "\n",
            "\n",
            "\n",
            "******************************\n",
            "gemma-1.1-7b-it question-guided em\n",
            "----------------\n",
            "Explicit:  30.864197530864196 1.069167165165972\n",
            "Implicit:  8.1799591002045 1.0626078574042188\n",
            "Scattered:  1.2544802867383513 0.3104033705320568\n",
            "\n",
            "******************************\n",
            "Mixtral-8x7B-Instruct question-guided em\n",
            "----------------\n",
            "Explicit:  31.069958847736622 0.17819452752766304\n",
            "Implicit:  7.566462167689162 0.20449897750511245\n",
            "Scattered:  2.6881720430107525 0.0\n",
            "\n",
            "******************************\n",
            "Llama-3-8B-Instruct question-guided em\n",
            "----------------\n",
            "Explicit:  30.65843621399177 0.17819452752765985\n",
            "Implicit:  8.588957055214724 0.0\n",
            "Scattered:  1.6129032258064515 0.0\n",
            "\n",
            "******************************\n",
            "Llama-3-70B-Instruct question-guided em\n",
            "----------------\n",
            "Explicit:  41.1522633744856 0.890972637638312\n",
            "Implicit:  8.452624403544649 0.47227015884631957\n",
            "Scattered:  1.9713261648745521 0.31040337053205697\n",
            "\n",
            "******************************\n",
            "gpt-4o question-guided em\n",
            "----------------\n",
            "Explicit:  39.81481481481482 0.30864197530864335\n",
            "Implicit:  11.860940695296524 0.20449897750511314\n",
            "Scattered:  3.046594982078853 0.3104033705320568\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}